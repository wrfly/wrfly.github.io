<!DOCTYPE html>
<html lang='zh_CN'><head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='最近越来越懒了，或者说懒癌越来越重了。经历了很多东西，却懒得记下来，可能会在今年的年终总结中写上几笔吧。
这一篇文章想说一下Prometheus的Metrics，聊一聊这一年来总结的经验。记得刚毕业的时候面过头条，面试官也问过我，说在百万级别的容器面前，metrics监控能有哪些，当时我支支吾吾说不上来，脑子里浮现的只有CPU，内存，带宽和IO。通过这一年多来不断对我们ads服务的完善，我对这个问题有了更好的答案。
Metrics 这一部分讲一下有哪些基本的metrics
QPS 首先能想到的就是QPS，但QPS其实是比较笼统的概念，到底是谁的QPS，在哪儿的QPS，请求的QPS还是返回的QPS，请求A的QPS还是请求B的QPS，是印尼的QPS还是马来的QPS，是serverA的QPS还是serverD的QPS？
metrics的目的是更好的了解系统状态，所以，你想知道多少的状态，就有多少的metrics，metrics一定是要有用的，而不是拍脑袋想出来的。
所以通过解决上述问题，针对某一个服务，大体能得出一些可用的metrics：
 整个服务调用的QPS：对系统的整体把控，变化应该均匀，除大促外，不应该有peak  调用方有哪些：看看是谁调用的最多，印尼还是马来？ 服务中每一个instance的QPS：流量是不是均衡  服务返回的QPS：变化也应该均匀，不然就是有某些instance超时了，没有及时响应  每个instance的返回的QPS：返回应该与请求一致  调用下游服务的QPS：如果这个服务不是单独存在，就需要知道它调用下游依赖的QPS，如果调用链是A-&gt;B-&gt;C, 则ABC的调用请求比例应一致 返回结果的QPS：拿我们ads来说，每秒返回多少广告应该是稳定的，不会出现突然升高或下降（压测，大促等人为情况除外），如果返回突然变少了，有可能是ES里的广告少了（误删了大批广告），有可能是instance处理不过来，请求10k，返回8k，2k超时（这时也会从调用端看出端倪，不单单是服务端告警）  返回成功的QPS：200, ok 返回失败的QPS：因为什么失败的 其他情况的QPS：过滤条件？  不同API的QPS：如果服务有多个API的话，需要针对每个API进行衡量，如果发现load变高，则能一眼看出是什么导致的，是不是某个API的请求变多了，还是下游某些服务变慢了，还是返回的数量便多了，等等等等。   在系统稳定的情况下，所有QPS变化都应该是平滑的，比例应该是一致的。
Error 错误也是系统中很重要的一个环节，正常情况下，不管是错误的数量还是比率，都应该很低。但是如果想精确定位问题，有两个点是需要考虑的。
一个是错误类型。golang里面我们可以针对 “可能会发生” 或者 “已经发生过” 的错误进行类型定义，判断error是不是某种类型，然后通过metrics导出，反映给grafana，并告警。如果错误没有发生过，则打印到error日志中，供后续分析，并不断提炼error类型，最终目标是将所有error都定义，不管是error code也好，error type也好，总之能够通过metrics反映哪里出了问题。
另一个是错误率。大多数情况下，我们对错误有一定的容忍度，并不是一旦出现error，就必须人工介入，有些error就是一直存在，隔几秒出现一次，在没有充足的时间和人力的情况下，我们倾向于ignore。所以就需要关心 “错误率”。假设对下游某种服务A的请求量是10k每秒，但是服务A不是那么稳定，100个请求里有1个是超时或者其他错误的，那么我们就会发现每秒的错误有100个，夜间会变成1个（100QPS的情况下）。这个时候，虽然错误的数量变化很大，但是这个是由请求数量的变化引起的，所以我们就可以忽略这种错误，等“有时间”了再去debug到底是哪里出了问题。
还有一点要特别注意的是，对于error qps的统计，我们也要分instance去看，有可能某些instance的网络出了问题，查询cache或者连接下游服务特别慢，也会导致大量error出现，这时候总体的error rate是上升的，但是对于instance而言，只有几个别出现了问题，并不是全部的instance都出了问题。
关于error的类型，很多时候都要根据自己的业务逻辑去定义，但是也有一些基础的error，比如：
 超时： context.DeadlineExceeded 调用某种接口X失败：其中包括各种RPC调用，发送Kafka消息，ACK消息； 缓存失败：Set，Get，NilResult&hellip; Invalid Request (fraud, limit, country, uid, session&hellip;) No Result DB error&hellip;  Latency 时延也是我们很关心的一个点，当某种服务的时延上升，一般来讲就是处理不过来了，这时候会影响下游所有的服务，一个超时，个个超时，从而导致服务完全不可用或者部分不可用。我们也可以通过调用端的时延来判断服务端的负载，还可以将client的时延和server的时延对比看，如果不一致，那么基本可以确定是网络出了问题。
所以，Latency的metrics有下：
 client latency：发起者的时延 server latency：接收者的处理时间 latency per instance：可以查看是不是某一台instance出了问题 ！！！ 如果请求与调用相关，还需要记录每种请求的latency，比如请求来源，请求大小，请求类型等 请求处理的总时间，包括所有的下游调用，缓存处理：因为我们很可能会疏漏某些调用服务，所以即使已有的latency表现一切正常，但是我们处理也有可能会变慢，这时通过总时间可以看到我们的响应变慢了，然后再一步步深追，到底是哪里的latency没有加，是哪里变慢了，是程序的问题还是系统的问题，等等 cache的latency：通常来讲，cache的GET操作不会超过1ms，但极端情况如大促，key或者value过大，cluster有网络问题，就会导致我们的cache变慢，这也会影响下游的服务调用。我们可以把cache当作一种服务来看待，归于client latency中  Cache 主要关心cache的hit rate和使用量，以及cache的latency，主要的Get，Set操作。'>
<meta name='theme-color' content='#ffcd00'>

<meta property='og:title' content='Prometheus-Metrics • wrfly&#39;s blog'>
<meta property='og:description' content='最近越来越懒了，或者说懒癌越来越重了。经历了很多东西，却懒得记下来，可能会在今年的年终总结中写上几笔吧。
这一篇文章想说一下Prometheus的Metrics，聊一聊这一年来总结的经验。记得刚毕业的时候面过头条，面试官也问过我，说在百万级别的容器面前，metrics监控能有哪些，当时我支支吾吾说不上来，脑子里浮现的只有CPU，内存，带宽和IO。通过这一年多来不断对我们ads服务的完善，我对这个问题有了更好的答案。
Metrics 这一部分讲一下有哪些基本的metrics
QPS 首先能想到的就是QPS，但QPS其实是比较笼统的概念，到底是谁的QPS，在哪儿的QPS，请求的QPS还是返回的QPS，请求A的QPS还是请求B的QPS，是印尼的QPS还是马来的QPS，是serverA的QPS还是serverD的QPS？
metrics的目的是更好的了解系统状态，所以，你想知道多少的状态，就有多少的metrics，metrics一定是要有用的，而不是拍脑袋想出来的。
所以通过解决上述问题，针对某一个服务，大体能得出一些可用的metrics：
 整个服务调用的QPS：对系统的整体把控，变化应该均匀，除大促外，不应该有peak  调用方有哪些：看看是谁调用的最多，印尼还是马来？ 服务中每一个instance的QPS：流量是不是均衡  服务返回的QPS：变化也应该均匀，不然就是有某些instance超时了，没有及时响应  每个instance的返回的QPS：返回应该与请求一致  调用下游服务的QPS：如果这个服务不是单独存在，就需要知道它调用下游依赖的QPS，如果调用链是A-&gt;B-&gt;C, 则ABC的调用请求比例应一致 返回结果的QPS：拿我们ads来说，每秒返回多少广告应该是稳定的，不会出现突然升高或下降（压测，大促等人为情况除外），如果返回突然变少了，有可能是ES里的广告少了（误删了大批广告），有可能是instance处理不过来，请求10k，返回8k，2k超时（这时也会从调用端看出端倪，不单单是服务端告警）  返回成功的QPS：200, ok 返回失败的QPS：因为什么失败的 其他情况的QPS：过滤条件？  不同API的QPS：如果服务有多个API的话，需要针对每个API进行衡量，如果发现load变高，则能一眼看出是什么导致的，是不是某个API的请求变多了，还是下游某些服务变慢了，还是返回的数量便多了，等等等等。   在系统稳定的情况下，所有QPS变化都应该是平滑的，比例应该是一致的。
Error 错误也是系统中很重要的一个环节，正常情况下，不管是错误的数量还是比率，都应该很低。但是如果想精确定位问题，有两个点是需要考虑的。
一个是错误类型。golang里面我们可以针对 “可能会发生” 或者 “已经发生过” 的错误进行类型定义，判断error是不是某种类型，然后通过metrics导出，反映给grafana，并告警。如果错误没有发生过，则打印到error日志中，供后续分析，并不断提炼error类型，最终目标是将所有error都定义，不管是error code也好，error type也好，总之能够通过metrics反映哪里出了问题。
另一个是错误率。大多数情况下，我们对错误有一定的容忍度，并不是一旦出现error，就必须人工介入，有些error就是一直存在，隔几秒出现一次，在没有充足的时间和人力的情况下，我们倾向于ignore。所以就需要关心 “错误率”。假设对下游某种服务A的请求量是10k每秒，但是服务A不是那么稳定，100个请求里有1个是超时或者其他错误的，那么我们就会发现每秒的错误有100个，夜间会变成1个（100QPS的情况下）。这个时候，虽然错误的数量变化很大，但是这个是由请求数量的变化引起的，所以我们就可以忽略这种错误，等“有时间”了再去debug到底是哪里出了问题。
还有一点要特别注意的是，对于error qps的统计，我们也要分instance去看，有可能某些instance的网络出了问题，查询cache或者连接下游服务特别慢，也会导致大量error出现，这时候总体的error rate是上升的，但是对于instance而言，只有几个别出现了问题，并不是全部的instance都出了问题。
关于error的类型，很多时候都要根据自己的业务逻辑去定义，但是也有一些基础的error，比如：
 超时： context.DeadlineExceeded 调用某种接口X失败：其中包括各种RPC调用，发送Kafka消息，ACK消息； 缓存失败：Set，Get，NilResult&hellip; Invalid Request (fraud, limit, country, uid, session&hellip;) No Result DB error&hellip;  Latency 时延也是我们很关心的一个点，当某种服务的时延上升，一般来讲就是处理不过来了，这时候会影响下游所有的服务，一个超时，个个超时，从而导致服务完全不可用或者部分不可用。我们也可以通过调用端的时延来判断服务端的负载，还可以将client的时延和server的时延对比看，如果不一致，那么基本可以确定是网络出了问题。
所以，Latency的metrics有下：
 client latency：发起者的时延 server latency：接收者的处理时间 latency per instance：可以查看是不是某一台instance出了问题 ！！！ 如果请求与调用相关，还需要记录每种请求的latency，比如请求来源，请求大小，请求类型等 请求处理的总时间，包括所有的下游调用，缓存处理：因为我们很可能会疏漏某些调用服务，所以即使已有的latency表现一切正常，但是我们处理也有可能会变慢，这时通过总时间可以看到我们的响应变慢了，然后再一步步深追，到底是哪里的latency没有加，是哪里变慢了，是程序的问题还是系统的问题，等等 cache的latency：通常来讲，cache的GET操作不会超过1ms，但极端情况如大促，key或者value过大，cluster有网络问题，就会导致我们的cache变慢，这也会影响下游的服务调用。我们可以把cache当作一种服务来看待，归于client latency中  Cache 主要关心cache的hit rate和使用量，以及cache的latency，主要的Get，Set操作。'>
<meta property='og:url' content='https://wrfly.kfd.me/posts/prometheus-metrics/'>
<meta property='og:site_name' content='wrfly&#39;s blog'>
<meta property='og:type' content='article'><meta property='article:section' content='posts'><meta property='article:tag' content='prometheus'><meta property='article:tag' content='experience'><meta property='article:published_time' content='2019-12-01T16:15:21&#43;08:00'/><meta property='article:modified_time' content='2019-12-01T16:15:21&#43;08:00'/><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.57.2" />

  <title>Prometheus-Metrics • wrfly&#39;s blog</title>
  <link rel='canonical' href='https://wrfly.kfd.me/posts/prometheus-metrics/'>
  
  
  <link rel='icon' href='/favicon.svg'>
<link rel='stylesheet' href='/assets/css/main.6a060eb7.css'><style>
:root{--color-accent:#ffcd00;}
</style>

<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-62244864-7', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

  

</head>
<body class='page type-posts'>

  <div class='site'><a class='screen-reader-text' href='#content'></a><div class='main'><nav id='main-menu' class='menu main-menu' aria-label=''>
  <div class='container'>
    
    <ul><li class='item'>
        <a href='/'>Home</a>
      </li><li class='item'>
        <a href='/tags/'>tags</a>
      </li><li class='item'>
        <a href='/posts/'>posts</a>
      </li><li class='item'>
        <a href='/links/'>links</a>
      </li><li class='item'>
        <a href='/about/'>About</a>
      </li><li class='item'>
        <a href='/repos/'>Repos</a>
      </li></ul>
  </div>
</nav><div class='header-widgets'>
        <div class='container'></div>
      </div>

      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>wrfly&#39;s blog</p><p class='desc site-desc'>使生如夏花之绚烂，死如秋叶之静美</p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='zh_CN' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Prometheus-Metrics</h1>
      

    </div>
    <div class='entry-meta'>
  <span class='posted-on'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>
<span class='screen-reader-text'> </span>
  <time class='entry-date' datetime='2019-12-01T16:15:21&#43;08:00'>2019, Dec 01</time>
</span>

  
  

</div>


  </div>
</header>

  
  

  <div class='container entry-content'>
  

<p>最近越来越懒了，或者说懒癌越来越重了。经历了很多东西，却懒得记下来，可能会在今年的年终总结中写上几笔吧。</p>

<hr />

<p>这一篇文章想说一下Prometheus的Metrics，聊一聊这一年来总结的经验。记得刚毕业的时候面过头条，面试官也问过我，说在百万级别的容器面前，metrics监控能有哪些，当时我支支吾吾说不上来，脑子里浮现的只有CPU，内存，带宽和IO。通过这一年多来不断对我们ads服务的完善，我对这个问题有了更好的答案。</p>

<h2 id="metrics">Metrics</h2>

<p>这一部分讲一下有哪些基本的metrics</p>

<h3 id="qps">QPS</h3>

<p>首先能想到的就是QPS，但QPS其实是比较笼统的概念，到底是谁的QPS，在哪儿的QPS，请求的QPS还是返回的QPS，请求A的QPS还是请求B的QPS，是印尼的QPS还是马来的QPS，是serverA的QPS还是serverD的QPS？</p>

<p>metrics的目的是更好的了解系统状态，所以，你想知道多少的状态，就有多少的metrics，metrics一定是要有用的，而不是拍脑袋想出来的。</p>

<p>所以通过解决上述问题，针对某一个服务，大体能得出一些可用的metrics：</p>

<ol>
<li>整个服务调用的QPS：对系统的整体把控，变化应该均匀，除大促外，不应该有peak

<ul>
<li>调用方有哪些：看看是谁调用的最多，印尼还是马来？</li>
<li>服务中每一个instance的QPS：流量是不是均衡</li>
</ul></li>
<li>服务返回的QPS：变化也应该均匀，不然就是有某些instance超时了，没有及时响应

<ul>
<li>每个instance的返回的QPS：返回应该与请求一致</li>
</ul></li>
<li>调用下游服务的QPS：如果这个服务不是单独存在，就需要知道它调用下游依赖的QPS，如果调用链是A-&gt;B-&gt;C, 则ABC的调用请求比例应一致</li>
<li>返回结果的QPS：拿我们ads来说，每秒返回多少广告应该是稳定的，不会出现突然升高或下降（压测，大促等人为情况除外），如果返回突然变少了，有可能是ES里的广告少了（误删了大批广告），有可能是instance处理不过来，请求10k，返回8k，2k超时（这时也会从调用端看出端倪，不单单是服务端告警）

<ul>
<li>返回成功的QPS：200, ok</li>
<li>返回失败的QPS：因为什么失败的</li>
<li>其他情况的QPS：过滤条件？</li>
</ul></li>
<li>不同API的QPS：如果服务有多个API的话，需要针对每个API进行衡量，如果发现load变高，则能一眼看出是什么导致的，是不是某个API的请求变多了，还是下游某些服务变慢了，还是返回的数量便多了，等等等等。
<br /></li>
</ol>

<p>在系统稳定的情况下，所有QPS变化都应该是平滑的，比例应该是一致的。</p>

<h3 id="error">Error</h3>

<p>错误也是系统中很重要的一个环节，正常情况下，不管是错误的数量还是比率，都应该很低。但是如果想精确定位问题，有两个点是需要考虑的。</p>

<p>一个是错误类型。golang里面我们可以针对 “可能会发生” 或者 “已经发生过” 的错误进行类型定义，判断error是不是某种类型，然后通过metrics导出，反映给grafana，并告警。如果错误没有发生过，则打印到error日志中，供后续分析，并不断提炼error类型，最终目标是将所有error都定义，不管是error code也好，error type也好，总之能够通过metrics反映哪里出了问题。</p>

<p>另一个是错误率。大多数情况下，我们对错误有一定的容忍度，并不是一旦出现error，就必须人工介入，有些error就是一直存在，隔几秒出现一次，在没有充足的时间和人力的情况下，我们倾向于ignore。所以就需要关心 “错误率”。假设对下游某种服务A的请求量是10k每秒，但是服务A不是那么稳定，100个请求里有1个是超时或者其他错误的，那么我们就会发现每秒的错误有100个，夜间会变成1个（100QPS的情况下）。这个时候，虽然错误的数量变化很大，但是这个是由请求数量的变化引起的，所以我们就可以忽略这种错误，等“有时间”了再去debug到底是哪里出了问题。</p>

<p>还有一点要特别注意的是，对于error qps的统计，我们也要分instance去看，有可能某些instance的网络出了问题，查询cache或者连接下游服务特别慢，也会导致大量error出现，这时候总体的error rate是上升的，但是对于instance而言，只有几个别出现了问题，并不是全部的instance都出了问题。</p>

<p>关于error的类型，很多时候都要根据自己的业务逻辑去定义，但是也有一些基础的error，比如：</p>

<ol>
<li>超时： <code>context.DeadlineExceeded</code></li>
<li>调用某种接口<code>X</code>失败：其中包括各种RPC调用，发送Kafka消息，ACK消息；</li>
<li>缓存失败：Set，Get，NilResult&hellip;</li>
<li>Invalid Request (fraud, limit, country, uid, session&hellip;)</li>
<li>No Result</li>
<li>DB error&hellip;</li>
</ol>

<h3 id="latency">Latency</h3>

<p>时延也是我们很关心的一个点，当某种服务的时延上升，一般来讲就是处理不过来了，这时候会影响下游所有的服务，一个超时，个个超时，从而导致服务完全不可用或者部分不可用。我们也可以通过调用端的时延来判断服务端的负载，还可以将client的时延和server的时延对比看，如果不一致，那么基本可以确定是网络出了问题。</p>

<p>所以，Latency的metrics有下：</p>

<ol>
<li>client latency：发起者的时延</li>
<li>server latency：接收者的处理时间</li>
<li>latency per instance：可以查看是不是某一台instance出了问题 <strong>！！！</strong></li>
<li>如果请求与调用相关，还需要记录每种请求的latency，比如请求来源，请求大小，请求类型等</li>
<li>请求处理的总时间，包括所有的下游调用，缓存处理：因为我们很可能会疏漏某些调用服务，所以即使已有的latency表现一切正常，但是我们处理也有可能会变慢，这时通过总时间可以看到我们的响应变慢了，然后再一步步深追，到底是哪里的latency没有加，是哪里变慢了，是程序的问题还是系统的问题，等等</li>
<li>cache的latency：通常来讲，cache的GET操作不会超过1ms，但极端情况如大促，key或者value过大，cluster有网络问题，就会导致我们的cache变慢，这也会影响下游的服务调用。我们可以把cache当作一种服务来看待，归于<code>client latency</code>中</li>
</ol>

<h3 id="cache">Cache</h3>

<p>主要关心cache的hit rate和使用量，以及cache的latency，主要的Get，Set操作。</p>

<p>不同的cache要不同对待，保证命中率不变，调整缓存时间。还是拿我们ads来举例，有些类型的广告，cache命中率高达70%，而有些没人看的广告，命中率只有20%，这个时候就需要调整他们的缓存时间，以达到资源利用率的最大化和展示结果的最优。通过经验动态调整缓存时间，在保证hit rate不变的情况下：提高缓存时间，就意味着对后端请求更少；减小缓存时间，就意味着返回的结果更及时，更新。</p>

<h3 id="go-metrcs">Go Metrcs</h3>

<ul>
<li>memory

<ul>
<li>alloc</li>
<li>gc</li>
<li>in use</li>
</ul></li>
<li>goroutine</li>
</ul>

<p>这些基本的监控很能反映程序运行状况，尤其是在有bug的时候，memory和goroutine会疯涨。一般来讲这种bug QA是测不出来的，只能在大流量的时候才会反映出来，但有时候上面这些的latency也会轻微受影响，因为系统很繁忙，这个时候就需要check一下是不是有goroutine泄漏，比如<strong>有没有close的channel导致goroutine无法退出</strong>？</p>

<h3 id="bussiness">Bussiness</h3>

<p>另外就是一些业务相关的监控了，比如我们广告的点击，收入，扣费状态，活跃的广告数量等等。通过对这些指标的监控，我们可以及时的知道系统有没有<strong>大</strong>问题，如果出现了断崖式的下降，那么肯定有些服务出了问题，再结合上面的metrics，定位原因并及时修复。</p>

<p>业务层面的监控和告警是结果导向型的，会有滞后性，所以一旦通过业务告警发现问题了，也代表我们系统的监控有疏漏，需要及时查缺补漏，在相应的服务中加上对应的监控和告警。尽量通过error来发现问题，而不是收入的下降。</p>

<h2 id="exporters">Exporters</h2>

<h3 id="命名">命名</h3>

<p>我是一个很吹毛求疵的人，对命名有着极其严格的要求，这也导致了自己看到别人的代码有时候会产生一种厌恶之情：这tm是啥意思？所以只要一眼看不出这个变量是干啥的名字，都是垃圾名字。</p>

<p>metrics的名字同样如此，好的metric是清晰明了的，甚至不需要额外注释就可以明白它是干嘛的。所以，如果你把握不好如何给metrics命名，就从PM或者QA或者旁边的同事拉个人过来问一下：“老哥，你看到这个名字第一反应是什么，能理解它是干啥的不？”，如果那个同事水平还可以的话（理解能力在线），给出的答案就是你能否用这个名字的答案。如果不相信这个哥们儿，就再拉一个人问问。</p>

<p>因为metrics的名字一旦确定，就相当于在墙上钉了个钉子，后期更改很麻烦，需要改grafana的图版，更改tag list同理。所以，这东西要慎重再慎重，三思而后行。</p>

<h3 id="代码">代码</h3>

<p>代码部分，prometheus的官方文档已经说的很详细了，在这里赘述实在没意思。</p>

<p>有两点tips是：</p>

<ol>
<li>在每个package下都放一个 <code>exporter.go</code> 或者 <code>metrics.go</code>，exporter全部小写，init函数注册到prometheus</li>
<li>如果exporter的tag过多，最好写一个函数，把<code>exporter.WithLableValues(...string)</code>包起来，对外只用包起来的这个函数，比如 <code>requestInc(country string, index int, err bool)</code>，这样会防止绝大多数的panic。exporter写的又臭又长对谁都没好处。</li>
</ol>

<h2 id="alert">Alert</h2>

<p>监控不是最终目的，我们添加监控的原因是找出异常并及时<strong>告警</strong>。</p>

<p>在阐述了上面的metrics之后，我想你应该知道哪里需要告警了：</p>

<ol>
<li>QPS 上升：某个instance，某个国家，某个xx</li>
<li>Latency 过高：同上</li>
<li>Error Rate 过高：error 类型</li>
<li>Revenue 下降</li>
<li>Memory/Goroutine 过高</li>
<li>返回结果变少</li>
</ol>

<p>需要注意的是，“上升”和“下降”都是同比前x分钟的，因为一天中我们的请求不可能保持不变（除非为零！！ 也需要告警！！），所以可以用一个公式来计算 increase 或者 drop 的 ratio，再把这个值次方放大，根据过往经验设置一个threshold，发出告警。一般来讲，相邻十分钟的数字不会差别很大，正常情况下会在 <code>1.0</code> 左右徘徊，如果发现了 <code>0.1</code> 或者 <code>10</code> 这样的值，那么恭喜你，系统可能出问题了。</p>

<p>至于latency，则可以根据平时的数字去调整告警阈值，过高或过低都不好。</p>

<p>除此之外还有：</p>

<ol>
<li>Server Restart (uptime 太小)</li>
<li>Server Down (应该有8台却只有7个metric)</li>
<li>Kafka Lag</li>
<li>Server CPU/Mem/Network/IO &hellip;</li>
</ol>

<hr />

<p>P.S. 写东西还是很耗精力的，下午睡了一觉，晚上还上了一版hotfix，晚饭吃的煮牛肉，挺管饱的还，一天没出门。现在是晚上十一点十四分，穿个衣服去超市买点东西了。</p>

<p>P.P.S. 希望以后的自己看到这篇文章能够指出疏漏和不足并补充新的想法。</p>

</div>

  
<footer class='entry-footer'>
  <div class='container sep-before'><div class='categories'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M22,19a2,2,0,0,1-2,2H4a2,2,0,0,1-2-2V5A2,2,0,0,1,4,3H9l2,3h9a2,2,0,0,1,2,2Z"/>
  
</svg>
<span class='screen-reader-text'>categories: </span><a class='category' href='/categories/golang/'>golang</a>, <a class='category' href='/categories/share/'>share</a></div>
<div class='tags'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M20.59,13.41l-7.17,7.17a2,2,0,0,1-2.83,0L2,12V2H12l8.59,8.59A2,2,0,0,1,20.59,13.41Z"/>
  <line x1="7" y1="7" x2="7" y2="7"/>
  
</svg>
<span class='screen-reader-text'>tags: </span><a class='tag' href='/tags/prometheus/'>prometheus</a>, <a class='tag' href='/tags/experience/'>experience</a></div>

  </div>
</footer>


</article>

<nav class='entry-nav'>
  <div class='container'><div class='prev-entry sep-before'>
      <a href='/posts/mendel-ab-testing-platform-p1/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 </span>
        <span class='screen-reader-text'>: </span>Mendel - A/B Testing Platform (Part 1)</a>
    </div></div>
</nav>


<section id='comments' class='comments'>
  <div class='container sep-before'>
    <div class='comments-area'><div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "wrfly" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>
  </div>
</section>

      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'><section class='widget widget-social_menu sep-after'><nav aria-label=''>
    <ul><li>
        <a href='https://github.com/wrfly' target='_blank' rel='noopener'>
          <span class='screen-reader-text'></span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
  
</svg>
</a>
      </li><li>
        <a href='mailto:mr.wrfly@gmail.com' target='_blank' rel='noopener'>
          <span class='screen-reader-text'></span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
  <polyline points="22,6 12,13 2,6"/>
  
</svg>
</a>
      </li><li>
        <a href='https://t.me/wrfly' target='_blank' rel='noopener'>
          <span class='screen-reader-text'></span><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <path d="m 22.05,1.577 c -0.393,-0.016 -0.784,0.08 -1.117,0.235 -0.484,0.186 -4.92,1.902 -9.41,3.64 C 9.263,6.325 7.005,7.198 5.267,7.867 3.53,8.537 2.222,9.035 2.153,9.059 c -0.46,0.16 -1.082,0.362 -1.61,0.984 -0.79581202,1.058365 0.21077405,1.964825 1.004,2.499 1.76,0.564 3.58,1.102 5.087,1.608 0.556,1.96 1.09,3.927 1.618,5.89 0.174,0.394 0.553,0.54 0.944,0.544 l -0.002,0.02 c 0,0 0.307,0.03 0.606,-0.042 0.3,-0.07 0.677,-0.244 1.02,-0.565 0.377,-0.354 1.4,-1.36 1.98,-1.928 l 4.37,3.226 0.035,0.02 c 0,0 0.484,0.34 1.192,0.388 0.354,0.024 0.82,-0.044 1.22,-0.337 0.403,-0.294 0.67,-0.767 0.795,-1.307 0.374,-1.63 2.853,-13.427 3.276,-15.38 L 23.676,4.725 C 23.972,3.625 23.863,2.617 23.18,2.02 22.838,1.723 22.444,1.593 22.05,1.576 Z"/>
  
</svg>
</a>
      </li></ul>
  </nav>
</section><div class='copyright'>
  <p> &copy; 2014-2019 wrfly </p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__assets_js_src="/assets/js/"</script>

<script src='/assets/js/main.67d669ac.js'></script>

</body>

</html>

