<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>share on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/categories/share/</link>
    <description>Recent content in share on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sun, 01 Dec 2019 16:15:21 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/categories/share/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Prometheus-Metrics</title>
      <link>https://wrfly.kfd.me/posts/prometheus-metrics/</link>
      <pubDate>Sun, 01 Dec 2019 16:15:21 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/prometheus-metrics/</guid>
      <description>最近越来越懒了，或者说懒癌越来越重了。经历了很多东西，却懒得记下来，可能会在今年的年终总结中写上几笔吧。
这一篇文章想说一下Prometheus的Metrics，聊一聊这一年来总结的经验。记得刚毕业的时候面过头条，面试官也问过我，说在百万级别的容器面前，metrics监控能有哪些，当时我支支吾吾说不上来，脑子里浮现的只有CPU，内存，带宽和IO。通过这一年多来不断对我们ads服务的完善，我对这个问题有了更好的答案。
Metrics 这一部分讲一下有哪些基本的metrics
QPS 首先能想到的就是QPS，但QPS其实是比较笼统的概念，到底是谁的QPS，在哪儿的QPS，请求的QPS还是返回的QPS，请求A的QPS还是请求B的QPS，是印尼的QPS还是马来的QPS，是serverA的QPS还是serverD的QPS？
metrics的目的是更好的了解系统状态，所以，你想知道多少的状态，就有多少的metrics，metrics一定是要有用的，而不是拍脑袋想出来的。
所以通过解决上述问题，针对某一个服务，大体能得出一些可用的metrics：
 整个服务调用的QPS：对系统的整体把控，变化应该均匀，除大促外，不应该有peak  调用方有哪些：看看是谁调用的最多，印尼还是马来？ 服务中每一个instance的QPS：流量是不是均衡  服务返回的QPS：变化也应该均匀，不然就是有某些instance超时了，没有及时响应  每个instance的返回的QPS：返回应该与请求一致  调用下游服务的QPS：如果这个服务不是单独存在，就需要知道它调用下游依赖的QPS，如果调用链是A-&amp;gt;B-&amp;gt;C, 则ABC的调用请求比例应一致 返回结果的QPS：拿我们ads来说，每秒返回多少广告应该是稳定的，不会出现突然升高或下降（压测，大促等人为情况除外），如果返回突然变少了，有可能是ES里的广告少了（误删了大批广告），有可能是instance处理不过来，请求10k，返回8k，2k超时（这时也会从调用端看出端倪，不单单是服务端告警）  返回成功的QPS：200, ok 返回失败的QPS：因为什么失败的 其他情况的QPS：过滤条件？  不同API的QPS：如果服务有多个API的话，需要针对每个API进行衡量，如果发现load变高，则能一眼看出是什么导致的，是不是某个API的请求变多了，还是下游某些服务变慢了，还是返回的数量便多了，等等等等。   在系统稳定的情况下，所有QPS变化都应该是平滑的，比例应该是一致的。
Error 错误也是系统中很重要的一个环节，正常情况下，不管是错误的数量还是比率，都应该很低。但是如果想精确定位问题，有两个点是需要考虑的。
一个是错误类型。golang里面我们可以针对 “可能会发生” 或者 “已经发生过” 的错误进行类型定义，判断error是不是某种类型，然后通过metrics导出，反映给grafana，并告警。如果错误没有发生过，则打印到error日志中，供后续分析，并不断提炼error类型，最终目标是将所有error都定义，不管是error code也好，error type也好，总之能够通过metrics反映哪里出了问题。
另一个是错误率。大多数情况下，我们对错误有一定的容忍度，并不是一旦出现error，就必须人工介入，有些error就是一直存在，隔几秒出现一次，在没有充足的时间和人力的情况下，我们倾向于ignore。所以就需要关心 “错误率”。假设对下游某种服务A的请求量是10k每秒，但是服务A不是那么稳定，100个请求里有1个是超时或者其他错误的，那么我们就会发现每秒的错误有100个，夜间会变成1个（100QPS的情况下）。这个时候，虽然错误的数量变化很大，但是这个是由请求数量的变化引起的，所以我们就可以忽略这种错误，等“有时间”了再去debug到底是哪里出了问题。
还有一点要特别注意的是，对于error qps的统计，我们也要分instance去看，有可能某些instance的网络出了问题，查询cache或者连接下游服务特别慢，也会导致大量error出现，这时候总体的error rate是上升的，但是对于instance而言，只有几个别出现了问题，并不是全部的instance都出了问题。
关于error的类型，很多时候都要根据自己的业务逻辑去定义，但是也有一些基础的error，比如：
 超时： context.DeadlineExceeded 调用某种接口X失败：其中包括各种RPC调用，发送Kafka消息，ACK消息； 缓存失败：Set，Get，NilResult&amp;hellip; Invalid Request (fraud, limit, country, uid, session&amp;hellip;) No Result DB error&amp;hellip;  Latency 时延也是我们很关心的一个点，当某种服务的时延上升，一般来讲就是处理不过来了，这时候会影响下游所有的服务，一个超时，个个超时，从而导致服务完全不可用或者部分不可用。我们也可以通过调用端的时延来判断服务端的负载，还可以将client的时延和server的时延对比看，如果不一致，那么基本可以确定是网络出了问题。
所以，Latency的metrics有下：
 client latency：发起者的时延 server latency：接收者的处理时间 latency per instance：可以查看是不是某一台instance出了问题 ！！！ 如果请求与调用相关，还需要记录每种请求的latency，比如请求来源，请求大小，请求类型等 请求处理的总时间，包括所有的下游调用，缓存处理：因为我们很可能会疏漏某些调用服务，所以即使已有的latency表现一切正常，但是我们处理也有可能会变慢，这时通过总时间可以看到我们的响应变慢了，然后再一步步深追，到底是哪里的latency没有加，是哪里变慢了，是程序的问题还是系统的问题，等等 cache的latency：通常来讲，cache的GET操作不会超过1ms，但极端情况如大促，key或者value过大，cluster有网络问题，就会导致我们的cache变慢，这也会影响下游的服务调用。我们可以把cache当作一种服务来看待，归于client latency中  Cache 主要关心cache的hit rate和使用量，以及cache的latency，主要的Get，Set操作。</description>
    </item>
    
    <item>
      <title>Discover-sync.Pool</title>
      <link>https://wrfly.kfd.me/posts/discover-sync.pool/</link>
      <pubDate>Tue, 20 Feb 2018 20:05:06 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/discover-sync.pool/</guid>
      <description>其实很久之前就用到了这个东西，起因是collecter程序占用太多内存了，然后就用sync.Pool复用额外消耗的一次性内存，避免GC周期太长使内存来不及释放而导致的OOM。
https://golang.org/pkg/sync/#Pool
简单的说，就是在一个池子里放了一些“东西”，这些东西是某种特殊的类型，用的时候需要指定。
池子会随着你的取用而扩张，比如说，池子里面放了扳手（扳手池），现在有10个工人依次取用，当第一个人取的时候，“扳手池”发现没有扳手，ok，new一个出来; 当第一个工人用完了的时候，把扳手放回扳手池，然后第二个人取的时候，扳手池就直接返回那个扳手就可以了。嗯……如果第一个工人没有归还呢，那么扳手池就要重新new一个扳手了，也就是这种情况：10个工人同时取用扳手，那么扳手池就得new10个新的出来了。
一个测试：
https://gist.github.com/wrfly/7de7f1e0c87860aa2f92dc6ed64cb75b
Makefile：
.PHONY: build run test NAME := $(shell basename `pwd`) build: go build run: ./$(NAME) test: build run go tool pprof -lines $(NAME) mem.porf  上面那个gist中，有几个测试，在这种情况下：
wg.Add(alloc) for i := 0; i &amp;lt; alloc; i++ { go func(num int) { // justMake() 	// bufPoolGet() 	bufPoolGetAndPut(num) // bufPoolSleepAndGetAndPut(num) 	wg.Done() }(i) } 也就是拿了接着放回去的时候，结果如下：
➜ syncPool_mem_usage_test make run go build ./syncPool_mem_usage_test newmake: 4 reused: 9996 reused slice(maybe equal to newmake) len: 0 ➜ syncPool_mem_usage_test  也就是说，新分配了4个1e6长度的[]byte，其余的都是复用的。</description>
    </item>
    
    <item>
      <title>RWMutex-and-sync.Map</title>
      <link>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</link>
      <pubDate>Sat, 03 Feb 2018 20:34:48 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</guid>
      <description>In the last post, I noted a problem of read-and-write in high-cocurrency situation and finally chose to use sync.Map. This post I will make a comparation between map with RWMutex and sync.Map.
Read-or-Write Test Code package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) type rwMap struct { data map[int]int m sync.RWMutex } var ( rwm = rwMap{data: make(map[int]int)} end = int(1e7) syncM sync.Map ) func main() { fmt.Println(&amp;#34;write test&amp;#34;) // rw map 	start := time.</description>
    </item>
    
    <item>
      <title>Kafka-Timestamp-and-Nginx-Udp-Proxy</title>
      <link>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</link>
      <pubDate>Wed, 24 Jan 2018 21:09:11 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</guid>
      <description>昨天遇到一个kafka lib的问题, 明明kafka版本是1.0, 发消息的时候也带上时间戳了, 但接收的时候就是看不到时间戳, timestamp为-1.
如果你也是用的sarama, 记着要在配置的时候制定kafka版本,不然就默认用最小的版本了, 而最小版本(0.8)是没有时间戳功能的, 所以即使后台的kafka是最新的1.0, 那也收不到timestamp, 因为sarama发的时候就没带, 协议版本里没有.
解决方法是, 指定kafka的版本:
config := sarama.NewConfig() config.Version = sarama.V1_0_0_0 // HERE! // sarama.WaitForLocal sarama.WaitForAll sarama.NoResponse config.Producer.RequiredAcks = conf.AckMode 还需要注意的一点是:
// Timestamp is the timestamp assigned to the message by the broker. This // is only guaranteed to be defined if the message was successfully // delivered, RequiredAcks is not NoResponse, and the Kafka broker is at // least version 0.</description>
    </item>
    
    <item>
      <title>Go-TrimLeft-and-TrimPrefix</title>
      <link>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</link>
      <pubDate>Wed, 24 Jan 2018 20:38:42 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</guid>
      <description>今天忽然发现了一个bug，其实是我自己的错误啦，不过也可以甩锅给文档……
简而言之，是用法错误，看这样一个例子：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/key&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }key Program exited. https://play.golang.org/p/Hn8-iVEUi-W
没毛病是吧？
如果你也觉着没毛病，那你也错啦！
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/sugar&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }ugar Program exited. https://play.golang.org/p/_-3V1PgLwjh
喂喂喂！怎么把我的sugar的s给吃了，这是bug吧！
嗯。的确是bug，不过，“凡事总要先从自身找原因”:
 func TrimLeft
func TrimLeft(s string, cutset string) string
TrimLeft returns a slice of the string s with all leading Unicode code points contained in cutset removed.</description>
    </item>
    
    <item>
      <title>Token-Bucket-II</title>
      <link>https://wrfly.kfd.me/posts/token-bucket-ii/</link>
      <pubDate>Sat, 20 Jan 2018 14:52:01 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/token-bucket-ii/</guid>
      <description>昨天公司年会上中了一部iPhoneX，感觉用尽了积攒了二十多年的运气。
 上回说到用ticker的方式后台fill令牌桶的效率是最高的，然后鄙人就很奇怪，所以就又刨根问底测试了一下，发现在单核的情况下（用docker的方式绑定CPU到容器），如果有大于100个协程在run的话，性能的确会受影响。
代码: https://gist.github.com/wrfly/3f2b23b20d53fbe5f9fee8e8f89fe861
CPU: Intel(R) Core(TM) i7-7600U CPU @ 2.80GHz
多核情况下的性能对比: ➜ ratelimit ./ratelimit 2018/01/20 15:10:28 start testing... 2018/01/20 15:10:28 5s test 2018/01/20 15:10:33 juju[lock]: 5.000108905 2018/01/20 15:10:33 take: 6000 2018/01/20 15:10:33 drop: 35740301 2018/01/20 15:10:33 2018/01/20 15:10:38 bsm[atomic]: 5.000146318 2018/01/20 15:10:38 take: 6000 2018/01/20 15:10:38 drop: 57423059 2018/01/20 15:10:38 2018/01/20 15:10:43 wrfly: 5.000100057 2018/01/20 15:10:43 take: 5000 2018/01/20 15:10:43 drop: 116287439 2018/01/20 15:10:43 2018/01/20 15:10:48 tb: 5.</description>
    </item>
    
    <item>
      <title>Token-Bucket</title>
      <link>https://wrfly.kfd.me/posts/token-bucket/</link>
      <pubDate>Sun, 14 Jan 2018 01:07:51 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/token-bucket/</guid>
      <description>上周解决的bug（其实并不能算是bug，是因为量太大导致的OOM）用到了令牌桶，简单在此记录一下。
关于令牌桶，最早接触的时候还是实习的时候用到的tc，这货限流就是用的令牌桶，简单地说，就是有一个桶，这个桶中有一些令牌，每进来一个包，就拿走一个令牌，当一定时间内（比如说，一秒）令牌被拿完了，那接下来的包就被丢弃了，因为他们没有拿到令牌；等到下一个时间周期，桶中再次被填满令牌，然后包来了再拿。
更详尽的参考：限流:漏桶算法和令牌桶算法  其中也有wikipedia的链接。
关于实现，github上搜到的golang的实现有这几个：
 https://github.com/bsm/ratelimit https://github.com/juju/ratelimit https://github.com/tsenart/tb  (P.S 还有我的hhh token bucket)
大体思路有这几个：
 使用lock+对比时间 使用原子操作+对比时间 使用原子操作+后台填桶（tikcer）  令牌桶其实很简单，就看怎样实现性能最好。
使用lock去拿token（看桶里还有没有token），然后再对比一下上次填桶的时间和取token的时间，就可以判断这个token能不能取到，这种实现的弊处在于lock的引入在数据量很大的情况下带来了不必要的lock和unlock，引起性能的下降。
所以就引出了第二种方案，使用原子操作来取代lock，性能有很大的提升。对比测试在下文中有代码。
可能会有人问，为什么要用对比时间的方式呢，后台用一个ticker定时填满不就行了。这也是我一开始想到的，但实际操作中却出现了问题，不知道是我姿势不对还是什么，测出的效果很差，怀疑是ticker没有定时执行，CPU没能调度到它上，没有来得及重置bucket。
然后就到了最后一种方式，后台填桶。这是最直白的，开一个goroutine每隔一段时间更新bucket中的token数量。但是，理论上在高并发的情况下，CPU繁忙，go出去的ticker容易失效（goroutine没能得到CPU资源），但实际情况却。。。大大超出我的想想，这种方法竟然是效率最高的！！！
对比测试：token-bucket-test.go
2018/01/13 18:05:17 5s test 2018/01/13 18:05:22 bsm: 5.000064716 2018/01/13 18:05:22 take: 6000 2018/01/13 18:05:22 drop: 54533531 2018/01/13 18:05:22 2018/01/13 18:05:28 wrfly: 5.000066249 2018/01/13 18:05:28 take: 4977 2018/01/13 18:05:28 drop: 57039560 2018/01/13 18:05:28 2018/01/13 18:05:34 tb: 5.000073633 2018/01/13 18:05:34 take: 6000 2018/01/13 18:05:34 drop: 113551687 2018/01/13 18:05:34 2018/01/13 18:05:35 range test: 100000000 2018/01/13 18:05:35 dry run: 0.</description>
    </item>
    
    <item>
      <title>Logrotate-Problem</title>
      <link>https://wrfly.kfd.me/posts/logrotate-problem/</link>
      <pubDate>Mon, 08 Jan 2018 20:59:37 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/logrotate-problem/</guid>
      <description>今天遇到一个logrotate的问题。
现象是配置不生效（其实也不是完全不生效，只是每小时的滚转策略变成一天了）。
配置文件：
# see &amp;#34;man logrotate&amp;#34; for details # rotate log files weekly weekly # keep 4 weeks worth of backlogs rotate 4 # create new (empty) log files after rotating old ones create # use date as a suffix of the rotated file dateext # uncomment this if you want your log files compressed #compress # packages drop log rotation information into this directory include /etc/logrotate.d$ ls -l /etc/cron.</description>
    </item>
    
    <item>
      <title>Share-Memory-by-Communicating</title>
      <link>https://wrfly.kfd.me/posts/share-memory-by-communicating/</link>
      <pubDate>Sat, 06 Jan 2018 20:39:56 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/share-memory-by-communicating/</guid>
      <description>Origin: https://blog.golang.org/share-memory-by-communicating
传统的多线程编程（比如Java，C++，Python等）需要码农通过在线程间共享内存的方式通信。一般来讲，共享的数据结构用锁来保护，线程想获取数据的时候必须先拿到锁。在某些情况下，用线程安全的数据结构能使其变得更加容易，比如Python的Queue。
Go的并发原语 - goroutine和channel - 提供了另一种优雅的方式去写并发软件。（这些概念有一个有趣的历史，起源于C. A. R. Hoare的 Communicating Sequential Processes）Go鼓励用channel的方式在goroutine之间传递引用数据，而不是声明一把锁去协调对共享数据的访问。这样的操作保证了在同一时刻只有一个goroutine拥有对数据的访问权。这个概念在高效Go编程（go程序员的必读文档）中有总结。
 不要通过共享的方式去沟通，通过沟通的方式共享内存。
 考虑这样一个程序，它轮旬一堆的URL。在传统的多线程编程中，你或许用以下的数据结构：
type Resource struct { url string polling bool lastPolled int64 } type Resources struct { data []*Resource lock *sync.Mutex } 然后有一个Poller的函数（并发执行）看起来可能会这样：
func Poller(res *Resources) { for { // get the least recently-polled Resource  // and mark it as being polled  res.lock.Lock() var r *Resource for _, v := range res.data { if v.</description>
    </item>
    
  </channel>
</rss>