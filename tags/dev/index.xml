<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>dev on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/tags/dev/</link>
    <description>Recent content in dev on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 30 May 2020 23:12:46 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/tags/dev/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>ML-Feature-Platform</title>
      <link>https://wrfly.kfd.me/posts/ml-feature-platform/</link>
      <pubDate>Sat, 30 May 2020 23:12:46 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/ml-feature-platform/</guid>
      <description>前言 大概从去年十二月份，领导们想做这样一件事情，重构一下算法组fetch feature的方法，当时是用的codis，用pb encode了一个大的结构体，结构体里包含了很多feature。当小组成员越来越多，模型越来越多，数据越来越多的时候，结构体里面的feature也越来越多，导致不管是添加新的feature还是删除不用的feature都非常麻烦，甚至都不知道哪些feature在用，哪些不用。渐渐的就变成了一座山。
然后我半路加入了讨论，一开始也是非常懵逼，都不知道他们在说什么，直到等听过一遍代码之后才有了大概的了解，开始着手设计新的方案。
大概花了一周的时间去设计：
 数据还是存在Redis里面，从Codis换成了Cache Cloud； 通过一个Config Server来  配置模型的feature，统一管理 配置Redis的地址，分国家，分feature类别 配置生成feature数据的job，包括数据源，写入地址等等   重构了生成日志的方式，这里的日志是“请求”和“返回的数据”，用来训练模型 重构了数据的存储方式，将大的pb结构拆成一个一个的hset，方便增删  设计上个人觉得没什么问题，领导也表示由我主导，但实际上我的话语权并不高，事情的发展也不在我的掌控之中，最终的效果就会有一些出入。更不用说不在我负责范围内的事情了，撇开不谈。
上周终于把全部国家的Redis资源申请到了，所以个人认为这件事情也有了一个阶段性成果（五个月？），所以记录一下这个项目和中间遇到的一些问题，希望能给以后的自己有所启发，或者反思。
思路 名词定义 Category - 表示一类的feature，比如Item Category，Shop Category，同一个Category中的QueryKey是相同的
QueryKey - 如何从Redis中读取feature，以及如何写入，其实就是Redis的Key。可以分为 ItemID，ShopID，UserID，QueryHash等等，以及他们之间的互相组合。
Config Server - 存储和管理Redis的地址，不同的Category不同的国家用哪个Redis，因为写入和读取都需要用到，但是写入和读取又不在同一个项目里，所以不能配置文件的方式保存。这个Server还会管理每个模型用到的feature set，跑map job的时候的配置，还有一些乱七八糟的东西。其实就是一个配置中心，字如其名。解耦用的。
Feature Set - 包含了要获取哪一些feature，这些feature分别在哪些category里。
问题 &amp;amp; 优化 1. 序列化 核心重构就是把存储的数据结构给换了，从pb structure转换成redis的kv，还是hset。因为client还是想拿到一个结构体，所以首先要面临的就是如何读取redis中的数据，并转换成相应的结构体的问题。
旧方案用 pb.Unmarshal 就可以了，但为了让数据读取更灵活（有些模型只需要pb结构里的某几个feature，但因为是整存整取的，所以只能全部取出，再挑出自己模型需要的数据，相应的就给redis带来了压力，不仅是带宽压力，更是Redis的IO压力，以及反序列化的CPU压力），我们就得转换从redis中读取到的KV。
说来也巧，一年前写过一个从环境变量转换成golang structure的一个lib，曾经给ES client用过，拿到这里来刚刚好，无非是数据源从env变成了redis kv，核心方法还是一样的，而且feature的格式是golang的基础类型，以及基础类型的array，完全支持。
kv -&amp;gt; struct 就算解决了。而且比json还要快。
2. Redis Key 因为我们的feature全是用redis key组织的，所以redis中就有非常多的key，此时key的大小就非常重要，能省一个字节，就省一个字节。
一开始是用 SET item:123456:name iPhone, SET item:123456:price 1000000 这种方式组织的，但Redis在实现key的时候，会额外占用一些空间，所以key越少越好，越紧凑越好，然后就演化成了 HSET item:12345 name iPhone, HSET item:12345 price 1000000, 但还是很大，我连这个 item， name， price 都不想要。于是又演化了一版，将这些定义存放到了Config Server里面，给每个category编了号，对应的，给每个category中的feature也编了号，这里唯一的缺陷是编号只能累加（或者删除旧的编号，人工赋值新的也行，但最后还是用autoincrease了事儿）。</description>
    </item>
    
    <item>
      <title>Mendel - A/B Testing Platform (Part 1)</title>
      <link>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p1/</link>
      <pubDate>Mon, 30 Sep 2019 00:41:31 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p1/</guid>
      <description>离上一篇文章已经过了好久了，能够记起还有这么一个坑没填，并且能够记得要些什么，纯属不易…… 这篇主要讲一下系统实现的时候遇到的问题以及我是如何解决的，当中会有疏漏的地方，也欢迎看到这篇文章并且感兴趣的人一起讨论。
流量分布 这个系统最最主要的目的和功能就是分配流量，所以如何分配流量，就成为了首先要面临的问题。
我们的配置分为两种，基础配置和实验配置，那么随之的，流量也分为两种：基础流量和实验流量。对于一个国家的请求而言，如果把流量按照一百份来划分，用了10%的流量做实验，那么就需要在这些请求中挑出10%出来，让其走实验的配置，剩余的90%则按照基础配置进行选择（在某个layer中运用某种算法）。
这其中有一个要求，即流量不能随机划分，不能让用户感知到变化，上一秒是根据base config进行的选择，出现了ABCD，下一秒也要按照base config，不能表现不一致。
这一点我们通过fnv哈希算法来实现，将用户的country，session和uid作为输入，确保三要素相同的情况下，得到的hash分布是随机的。
当然这里也有一些细节，比如既要支持按照session划分，又要按照uid划分，在对一致性要求没那么高的时候，还可以加上hour作为第四因素，变相放大实验流量，得到更多结果。
所以解决方案为：
fnv(country + session + uid [+ hour]) % 100 的出一个数字，通过这个数字判定是base还是experiment
实验选择 接着上面，在得到一个[0,99)的数字以后，怎么就能确定是base还是exp呢？
如果base的config占80%, exp1占10%，exp2占5%，exp3占5%，如何确定一个请求（hash = 55）在哪里呢？
最简单的方法，用一个map[uint8]string来存储没一个数字所对应的实验ID或者BaseID，每次请求过来都用这个map查一下。
但我有点嫌慢，就选择了另外一种方式：bitmap
我们有100份流量，但是数字最大到int64，所以就需要两个int64来存储我们的实验信息，如果一个数字是45,那么就选择第一个bitmap，如果是76,就选择第二个。
具体操作是将bitmap右移[&amp;gt;&amp;gt;]hash位，在与1与或[&amp;amp;&amp;amp;]，如果是真，那么就在这个实验里，如果是假，就换下一个实验的bitmap进行同样操作。
这样能够比map的方式快50%
流量分布不均匀 这其实是自己挖的一个坑，因为所有的流量都会分布到[0,99)上，所以我在存储的时候用了uint8, 认为这个数字已经足够用了，但实际上，虽然它够用，但模100之后并不平均，因为uint8的范围是[0,128), 将这一批平均分布的数字模100之后的分布则必然不平均，[0,28)在[0,99)内占了两份。
// Hash returns a hash value of [0,100) func Hash(x string) uint8 { hash := fnv.New32a() hash.Write([]byte(x)) return uint8(hash.Sum32()) % 100 } 所以简单粗暴的方式是更改一下取模顺序，我们认为hash.Sum32()是平均的，那么其mod 100也是平均的，则最后类型转换成uint8则也是平均的了。
// Hash returns a hash value of [0,100) func Hash(x string) uint8 { hash := fnv.</description>
    </item>
    
    <item>
      <title>Mendel - A/B Testing Platform (Part 0)</title>
      <link>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p0/</link>
      <pubDate>Sun, 07 Jul 2019 13:53:31 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p0/</guid>
      <description>背景 &amp;amp; 前言 其实很早之前就想重写我们的AB testing系统，因为现在的程序很简陋（虽然是很久之前某位大哥写的，而且也能凑合用），不支持分国家测试（这种测试很重要），每个返回的结果都会带上很长一串的配置代码（比如，Testing=abpha,Rank=abcde,Threshold=0.5&amp;hellip;.，巨长无比，比我都长），看起来非常罗嗦，也不友好，我们用什么测试都明文暴露出去了，并且现有程序很Java，晦涩难懂，不敢维护。本着破旧立新的原则，搜了一圈没有开源方案能够满足我们的要求，只得自己写一个。
历时两个月：写代码一个月，五千多行，期间还掺杂着乱七八糟的事儿；部署测试一个月，没有QA资源给我们测试，只能自己测，在mesos上部署还搞了一周，DB，DNS，deploy.json, 超级烦。
不过好歹，七月份终于上线了，又经过了两周的线上测试，修了几个bug，现在看起来还不错。
一些简单的功能：
 支持多个项目：不同项目有自己不同的配置和实验 支持多service绑定测试：同一个项目中有多个不同的service 配置/实验区分国家： 不同的国家有不同的配置/测试 可根据UserID，SessionID等维度划分实验流量 支持配置权重：同一个配置有多个值，值附带权重属性 配置多版本，实验多版本：不同版本同时保存，互不冲突 实时分析试验结果：不必麻烦数据团队隔夜处理数据，计算CTR  大概有36个API，因为没有前端（PM画了界面，但是没人有时间写），我还写了一个简单的Python Cli程序。对了，程序语言是Golang，当然。
我给系统起名叫 Mendel，但是PM的PRD上叫MVT，Multiple Variable Testing system，这个名字个人感觉很low，不如“孟德尔”有意思。不过我才是写代码的人，我想怎么写就怎么写（傲娇）。
架构 整个系统分为四个部分：
 Mendel Server Mendel Assistant Database  MySQL DB etcd influxDB   Mendel Lib  Mendel Server 负责处理所有API请求，基本上就是CURD，接受请求，验证处理，存入DB，同步到etcd上。这部分写起来其实很讨厌，虽然简单，但是太麻烦，每一个接口都要有CURD，我原来想用MongoDB作为后端存储，但是公司不提供MongoDB的维护，自己用容器风险又太大（没人维护），只能用MySQL，然后用text保存数据。因为如果想要满足设计，配置就需要相当复杂，区分国家，配置权重，service关系，不同配置的层级关系，用MySQL字段保存的话太麻烦了，也不能满足多种层级。其实写好一个CURD也不是容易事，太多细节需要考虑，不仅仅是麻烦的问题。
Mendel Assistant
这个组件是为了实时分析实验结果的，通过实时读取Tracking数据，获取Tracking里面的AB Testing Siganture，解开Signature，写入InfluxDB和暴露metrics给Prometheus，计算不同实验的CTR，对比效果。Mendel Server部署一个就够了，他只是个CURD Server，但这个Assistant需要部署很多个，毕竟Tracking的数据量太大了。
Database
MySQL，所有的“配置”都在这里，包括各个项目的元信息，基础配置，实验配置，流量分配（以后还会有用户管理，没来得及实现用户系统）
Etcd，也是存储各种的配置信息，不过它还有解藕client与db的功能，也是作为一个缓存的存在，也作为发布/更新配置与实验的中间件（归功与etcd v3的gRPC长连接watch）（唉，但是自从接手etcd以来，已经踩了好多坑了……）。
InfluxDB，存储我们的试验结果，因为其支持多个field，不像projetheus那样只能有固定个数的lables。需要多个fields的原因是，系统运行中，配置会随时更改，增加或者删除某一些层级配置，比如有时候想测试某种算法的好坏，但一旦测试完成，就有可能把这一层的配置删掉，固定用测试结果优秀的算法，或者动态更改配置的阈值，有时候是0.2,有时候是0.3,有时候是0.5,这都是不可预测的，所以只能用InfluxDB解决。且InfluxDB聚合能力还可以，虽然是单点（公司不给出钱买商业版），但我还没遇到什么瓶颈，如果数据丢了，重新set一下Kafka的Offset，再跑一边写进去就是了。
Mendel Lib
原来的设计是没一个请求进来，我们用一个TrackingID去标记，然后在TrackingID上append不同的配置，写入一个KV存储中，但是Lead说，公司的Codis不靠谱，没有一个靠谱的持久化KV存储，如果TrackingID的数据丢了就麻烦了，所以最后折中了一个Signature的方案，从很长的配置细节变成一个能够自解的signature，并且signature对外不可读。对于signature的介绍将在本文最后进行。
其实用trackingID的方式是最好的，这样不仅可以用来标记数据，也可以用这个ID对请求进行追踪，比如在什么时候产生了访问，返回了多少广告，关键词是什么，在什么时候点击的，点击了第几个位置，这个位置的广告的平均CTR是多少，rank分是多少，similarity是多少，这些数据对于我们分析用户行为，提升算法精准度，都是有很大帮助的，但是无奈，只能用signature去自解AB Testing的数据。无奈。
配置结构 在这套系统中，有几个关键的概念：
 project: 拥有独立流量（独立请求和返回结果）的业务线称为一个项目 base_config: 项目的基础配置，一个项目配置中，可以包含多个服务  service: 服务，一个服务配置中，包含多个模块配置 module:：模块，具体表现为程序的某个模块，可以包含多个层级配置 layer: 在程序中的特定层，具体为某一个模块中的某一个步骤 config value: 对层级的配置，包括这个层中，对国家的配置值，不同值的权重等   traffic slots: 每一个项目拥有自己独立的流量分配，每一个国家也有独立的流量分配（保证了实验的可以在国家的维度进行） experiment: 每一个实验本质上也是一套配置，不过是建立在基础配置之上的特定修改，而且实验配置不能超出基础配置，一定是修改。实验可以根据不同的维度划分，比如根据UID划分，根据SessionID划分，但是它们一定是共享整个项目的流量的，相同国家内的流量不能够重叠交叉。实验中对于layer的配置也可以有多种，且对于每一种配置值，其权重都是相同的（有别于base config中的config value）。对于实验没有修改的layer，当base config中layer有变更时，experiment也要同步进行修改，更新，下发。  对于base config和expeirment config，所有数据都有唯一的ID和版本号，保证了不会有数据丢失，方便回滚和审计。</description>
    </item>
    
    <item>
      <title>Etcd-Authentication</title>
      <link>https://wrfly.kfd.me/posts/etcd-authentication/</link>
      <pubDate>Thu, 16 May 2019 21:15:45 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/etcd-authentication/</guid>
      <description>起因 今早上我们的某个程序升级了，负责升级的哥们儿没有检查QPS就去睡觉了（当然检查了也没用，除了尽早发现bug然后把大家喊起来），然后我们的搜索出了问题（好久），直到上班才发现，然后重启解决（Thanks Dat）。
问题是啥？
升级后，新的instance没有请求，依赖这个服务的所有service都连着旧的地址（在mesos上重新部署之后地址和端口都变了），但这些instance都是通过etcd作服务发现的，把自己注册到etcd上，而且etcd上也的确看到了新的地址，但是为什么那些server还抓着旧地址不放呢？（讲道理我写的lib没问题啊）而且重启之后就能拿到新的地址了，也有请求到升级之后的instance上去了。
经过 Dat在群里贴了这样一段，取自etcd官方文档：
 At first, a client must create a gRPC connection only to authenticate its user ID and password. An etcd server will respond with an authentication reply. The reponse will be an authentication token on success or an error on failure. The client can use its authentication token to present its credentials to etcd when making API requests.
  The client connection used to request the authentication token is typically thrown away; it cannot carry the new token&amp;rsquo;s credentials.</description>
    </item>
    
    <item>
      <title>Golang-Open-File</title>
      <link>https://wrfly.kfd.me/posts/golang-open-file/</link>
      <pubDate>Tue, 16 Apr 2019 22:13:25 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-open-file/</guid>
      <description>很久之前遇到的问题了，清任务的时候这个write排第一，就写一下。
起因 要做一个临时的告警系统，又不想引入DB，也不想用bolt，不想引入额外的包，所以就傻了吧唧的自己用文件实现一个简单的DB。
（现在回想起来真的很没必要，老老实实用boltdb多好，简单快速搞得定）
每一条告警都有自己的hash，如果在文件中没有发现这条hash，则发送告警并将hash写入文件，如果发现了则跳过，目的是防止重复告警。
很简单的逻辑，但bug是，还是会发送重复的告警，也就是说，如果这条告警半小时之前发过了，那么半小时之后还会再发。（程序是用cron job跑的，每半小时跑一次，查询各个状态，如果指标低于阈值则告警）
经过 程序的某个部分是这样写的：
fileName := &amp;#34;/tmp/alert-history.txt&amp;#34; f, err := os.OpenFile(fileName, os.O_RDWR, 0644) if err != os.ErrNotExist { panic(fmt.Sprintf(&amp;#34;open file %s error: %s&amp;#34;, fileName, err)) } else { f, err = os.Create(fileName) if err != nil { panic(fmt.Sprintf(&amp;#34;create file %s error: %s&amp;#34;, fileName, err)) } } defer f.Close() // detect alarm 	// ...  // if alarm not in file 	// then send alarm and 	// write its hash to the file 	// .</description>
    </item>
    
    <item>
      <title>gRPC-LoadBalancer</title>
      <link>https://wrfly.kfd.me/posts/grpc-loadbalancer/</link>
      <pubDate>Sat, 16 Mar 2019 00:32:21 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/grpc-loadbalancer/</guid>
      <description>gRPC Load Balancer (with etcd) 引子 两个月之前，我们组想脱离公司全局的 Nginx 代理（毕竟 Nginx 的 TCP 代理用来做 gRPC 的负载均衡有很多问题），使用 gRPC 自带的负载均衡，调研了一圈开源的实现，有用consul的， 有用旧版本API实现的（方法将要被弃用），不得已，只能自己翻文档实现。
弊端 且说一下 Nginx 作为 gRPC 服务的负载均衡的问题，由于 Nginx 作为中间件，gRPC 的 client 是不知道后端有多少服务的，它只认 Nginx 的地址，所有的服务发现和负载均衡由Nginx统一处理， 默认的负载方式是轮询。
连接池 初始阶段，client 建立N多链接到 Nginx（N的数量取决于 pool 的大小，也就是人工实现一个连接池， 其实根本没必要，但为了让流量均衡的发送到后端所有服务上必须要这样），但这个N的数字其实很有讲究， 如果N小于后端服务的数量，那么后端真实server收到的请求是不均匀的，在仅有一个 client，10个 server 的情况下，如果 client 建立5条连接到 Nginx，然后 Nginx 分别转发给后端的每一个 server， 那么就会有5个 server 收不到连接，白干。如果增加 client的数量，2x5=10，后端的10个 server 都收到了请求，才均匀。
但，这个pool大小的设定实在玄学，client的数量也会变更，鬼知道哪些server收到的请求多，哪些 server收到的请求少？
又，gRPC 是建立在 TCP 上的，每条连接可处理的请求能到 1w+ qps 以上，client 端建立几十几百个 连接到Nginx，Nginx 又转发这些到后端的N个 server 实例，看起来实在很傻逼。
异常 server扩容 后端server总要扩容的，不管是大促还是活动，谁也不能保证不会增加server数量。但是在Nginx代理的情况下， 如果后端新增了 backend，由于是TCP代理，已有的连接不能掐断，但client端的连接池已经初始完毕， 不会继续建立连接到Nginx，这时候新上的server就收不到一丁点流量，直到新连接的建立。</description>
    </item>
    
    <item>
      <title>An-Accident-Triggered-by-a-String</title>
      <link>https://wrfly.kfd.me/posts/an-accident-triggered-by-a-string/</link>
      <pubDate>Tue, 22 Jan 2019 21:43:01 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/an-accident-triggered-by-a-string/</guid>
      <description>今天下午健身回来想着注册线上的search server到zk，让prometheus去拉metrics，然后由于申请机器权限的系统经常性的抽风，申请登录机器一直不成功（老牛鼻子问题了，他们就是不修），然后就想直接更新zk的数据，反正脚本也是这样，然后就到了consule的UI上（我也不知道为啥consul的UI跟ZK的数据是连着的），直接创建对应的key。
创建完成之后发现并没有卵用，过了一会儿就被打岔喊道了楼上去（现在想起来，如果不上去的话会不会自己就把key给删了？但大概率会忘掉！），说了一会儿之后正巧他们出故障了，隐隐约约听到了我们组的名字，妈的瞬间就精神了，这不是我刚才建的key吗，卧槽我是不是闯祸了？
仔细听下去，果真。日。
 事情的起因是我想注册我们的search server到ZK，让prometheus去拉数据。
经过是没有登录到注册zk的机器上，然后就手工写错了注册的path和value。
结果导致平台组负责服务发现的程序崩溃？
程序期待一个JSON，我写了一个string，然后程序就崩溃了。贰叁叁。
最终导致订单系统停止服务五十分钟。（讲道理五十分钟才解决这种故障也是……）
 然后群里领导们就在讨论如何避免以后类似问题的发生，说如何限制zk的写入，如何提交ticket，如何更新服务发现等等等等。
我就想，这难道没有work flow和programer的责任吗，每个人都能写线上的ZK不太好吧？这么简单的异常，程序没有正确处理而是直接崩溃不好吧？花费五十分钟还顺带回滚服务（当然没卵用了也）才解决问题也不好吧？最终我（导火索）如果没有恰好站在旁边背锅，也发现不了是怎么引起的bug也不太对吧？
洋洋洒洒都在讨论auth的问题，真是，醉了。
服务发现用ZK本来就值得吐槽了，而且各种服务都写ZK，不区分国家和服务类型，把ZK当银弹用，还是一颗，这么明显的问题都没人提出来，也没人说程序写的有问题，（这么关键的程序，感觉像是随便写写的），深深的觉着领导层有点本末倒置。
普林西普的刺杀引发了一战，但讲道理，让普林西普背一战的锅就说不过去。当然也没人让我背锅，我只是觉着导火索就不应该存在，把潜在的问题扼杀在摇篮中是最好的。因为这次可能是我写string，下次可能就是程序写int，在下次可能就是黑客直接递归删key了，暴露出这么多问题，一个auth能解决？一个disable能解决？
服务发现本来就很复杂，etcd都能被打挂，zk也不是万能钥匙啊，况且还不分区。这么关键的核心服务连这种异常都hold不住，也难怪天天修bug。
我肯定是要被批评的，因为我写的key嘛，就像上次99时候DNS挂了，一个更改防火墙的DBA背锅一样。
 几点经验教训：
 对核心数据的权限控制：什么样的人能干什么样的事儿，该干什么样的事儿要有明确的划分。 对数据的隔离和分区：不要把鸡蛋放在一个篮子里，老生常谈了。 核心程序的code review和至少80%的单元测试：不要让这么低级的异常把整个系统搞挂。 关键数据的备份要做好：实在解决不了问题咱回滚数据行不行？ 还是告警，还是日志，错误处理不是说说那么简单的。 服务发现是个大事儿：别用zk，用etcd。 哪怕是内部服务，也不能完全可信，尤其是带有网络调用的，数据验证是基本操作。 出问题是好事儿，跌倒也是好事儿，但出了问题不总结就不是好事儿，总结不到位也不是好事儿，就像明明是脚上扎了钉子，却一直想办法如何修路。早点暴露问题就能早点修复，以免在更严重的时期触发问题；早点跌倒就会早点反思，而不是拖着一只被扎穿的脚彳亍。 最重要的一点也是针对我的一点，没事儿别瞎搞，对于不了解的东西要如履薄冰，如坐针毡，没准你就是最后一块被抽掉的积木。  </description>
    </item>
    
    <item>
      <title>Read-and-Write-in-High-Concurrency</title>
      <link>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</link>
      <pubDate>Fri, 02 Feb 2018 19:17:28 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</guid>
      <description>问题 今天遇到一个高并发读写的问题。
具体的场景是，有一个“策略”的集合，然后每秒有很多消息进来，每一条消息都要匹配有没有对应的策略，如果有的话就应用策略（更改消息的某个属性），没有的话就返回。
抽象来看，就是在N多读的同时，怎样去写数据。
一开始我的方法是策略存在数组里，消息来了就去遍历数组，如果匹配到了就返回对应的规则。这种方法最笨，因为每一条消息过来，我都要去循环遍历整个数组，如果数组很长的话（有很多规则），那么带来的无谓开销会很大，复杂度为O(n)。
而且还一个问题，如果在range数组的时候，数组发生了变化，那么就会读到错误的值，或者崩溃。
一种解决的方法是，在遍历之前，首先拷贝一份新的，遍历新的策略数组，而不是原有的全局变量。这种方法的问题在于，每次匹配规则的时候，都要进行一次拷贝，虽然简单，也能解决问题，但，太浪费资源，而且很丑。
最终的思路是，用哈希表的方式去匹配策略，从复杂度上来看是O(1)的操作，但问题在于并发读写哈希表。
复现 为了更容易的表示问题，用了大量的并发读写（实际情况没有下面那么频繁，写操作要比读操作少得多得多）：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;time&amp;#34; ) var biu map[int]int func read() { for i := 0; i &amp;lt; len(biu); i++ { if biu[i] != i { fmt.Printf(&amp;#34;%d != %d\n&amp;#34;, biu[i], i) panic(&amp;#34;!&amp;#34;) } } } func write() { for i := 0; i &amp;lt; time.Now().Second(); i++ { biu[i] = i } } func main() { biu = make(map[int]int) go func() { for { go write() time.</description>
    </item>
    
    <item>
      <title>Kafka-Timestamp-and-Nginx-Udp-Proxy</title>
      <link>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</link>
      <pubDate>Wed, 24 Jan 2018 21:09:11 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</guid>
      <description>昨天遇到一个kafka lib的问题, 明明kafka版本是1.0, 发消息的时候也带上时间戳了, 但接收的时候就是看不到时间戳, timestamp为-1.
如果你也是用的sarama, 记着要在配置的时候制定kafka版本,不然就默认用最小的版本了, 而最小版本(0.8)是没有时间戳功能的, 所以即使后台的kafka是最新的1.0, 那也收不到timestamp, 因为sarama发的时候就没带, 协议版本里没有.
解决方法是, 指定kafka的版本:
config := sarama.NewConfig() config.Version = sarama.V1_0_0_0 // HERE! // sarama.WaitForLocal sarama.WaitForAll sarama.NoResponse config.Producer.RequiredAcks = conf.AckMode 还需要注意的一点是:
// Timestamp is the timestamp assigned to the message by the broker. This // is only guaranteed to be defined if the message was successfully // delivered, RequiredAcks is not NoResponse, and the Kafka broker is at // least version 0.</description>
    </item>
    
    <item>
      <title>Go-TrimLeft-and-TrimPrefix</title>
      <link>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</link>
      <pubDate>Wed, 24 Jan 2018 20:38:42 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</guid>
      <description>今天忽然发现了一个bug，其实是我自己的错误啦，不过也可以甩锅给文档……
简而言之，是用法错误，看这样一个例子：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/key&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) } key Program exited. https://play.golang.org/p/Hn8-iVEUi-W
没毛病是吧？
如果你也觉着没毛病，那你也错啦！
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/sugar&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) } ugar Program exited. https://play.golang.org/p/_-3V1PgLwjh
喂喂喂！怎么把我的sugar的s给吃了，这是bug吧！
嗯。的确是bug，不过，“凡事总要先从自身找原因”:
 func TrimLeft
  func TrimLeft(s string, cutset string) string
  TrimLeft returns a slice of the string s with all leading Unicode code points contained in cutset removed.</description>
    </item>
    
  </channel>
</rss>