<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Dev on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/tags/dev/</link>
    <description>Recent content in Dev on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 16 Mar 2019 00:32:21 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/tags/dev/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>gRPC-LoadBalancer</title>
      <link>https://wrfly.kfd.me/posts/grpc-loadbalancer/</link>
      <pubDate>Sat, 16 Mar 2019 00:32:21 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/grpc-loadbalancer/</guid>
      <description>gRPC Load Balancer (with etcd) 引子 两个月之前，我们组想脱离公司全局的 Nginx 代理（毕竟 Nginx 的 TCP 代理用来做 gRPC 的负载均衡有很多问题），使用 gRPC 自带的负载均衡，调研了一圈开源的实现，有用consul的， 有用旧版本API实现的（方法将要被弃用），不得已，只能自己翻文档实现。
弊端 且说一下 Nginx 作为 gRPC 服务的负载均衡的问题，由于 Nginx 作为中间件，gRPC 的 client 是不知道后端有多少服务的，它只认 Nginx 的地址，所有的服务发现和负载均衡由Nginx统一处理， 默认的负载方式是轮询。
连接池 初始阶段，client 建立N多链接到 Nginx（N的数量取决于 pool 的大小，也就是人工实现一个连接池， 其实根本没必要，但为了让流量均衡的发送到后端所有服务上必须要这样），但这个N的数字其实很有讲究， 如果N小于后端服务的数量，那么后端真实server收到的请求是不均匀的，在仅有一个 client，10个 server 的情况下，如果 client 建立5条连接到 Nginx，然后 Nginx 分别转发给后端的每一个 server， 那么就会有5个 server 收不到连接，白干。如果增加 client的数量，2x5=10，后端的10个 server 都收到了请求，才均匀。
但，这个pool大小的设定实在玄学，client的数量也会变更，鬼知道哪些server收到的请求多，哪些 server收到的请求少？
又，gRPC 是建立在 TCP 上的，每条连接可处理的请求能到 1w+ qps 以上，client 端建立几十几百个 连接到Nginx，Nginx 又转发这些到后端的N个 server 实例，看起来实在很傻逼。</description>
    </item>
    
    <item>
      <title>An-Accident-Triggered-by-a-String</title>
      <link>https://wrfly.kfd.me/posts/an-accident-triggered-by-a-string/</link>
      <pubDate>Tue, 22 Jan 2019 21:43:01 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/an-accident-triggered-by-a-string/</guid>
      <description>今天下午健身回来想着注册线上的search server到zk，让prometheus去拉metrics，然后由于申请机器权限的系统经常性的抽风，申请登录机器一直不成功（老牛鼻子问题了，他们就是不修），然后就想直接更新zk的数据，反正脚本也是这样，然后就到了consule的UI上（我也不知道为啥consul的UI跟ZK的数据是连着的），直接创建对应的key。
创建完成之后发现并没有卵用，过了一会儿就被打岔喊道了楼上去（现在想起来，如果不上去的话会不会自己就把key给删了？但大概率会忘掉！），说了一会儿之后正巧他们出故障了，隐隐约约听到了我们组的名字，妈的瞬间就精神了，这不是我刚才建的key吗，卧槽我是不是闯祸了？
仔细听下去，果真。日。
事情的起因是我想注册我们的search server到ZK，让prometheus去拉数据。
经过是没有登录到注册zk的机器上，然后就手工写错了注册的path和value。
结果导致平台组负责服务发现的程序崩溃？
程序期待一个JSON，我写了一个string，然后程序就崩溃了。贰叁叁。
最终导致订单系统停止服务五十分钟。（讲道理五十分钟才解决这种故障也是……）
然后群里领导们就在讨论如何避免以后类似问题的发生，说如何限制zk的写入，如何提交ticket，如何更新服务发现等等等等。
我就想，这难道没有work flow和programer的责任吗，每个人都能写线上的ZK不太好吧？这么简单的异常，程序没有正确处理而是直接崩溃不好吧？花费五十分钟还顺带回滚服务（当然没卵用了也）才解决问题也不好吧？最终我（导火索）如果没有恰好站在旁边背锅，也发现不了是怎么引起的bug也不太对吧？
洋洋洒洒都在讨论auth的问题，真是，醉了。
服务发现用ZK本来就值得吐槽了，而且各种服务都写ZK，不区分国家和服务类型，把ZK当银弹用，还是一颗，这么明显的问题都没人提出来，也没人说程序写的有问题，（这么关键的程序，感觉像是随便写写的），深深的觉着领导层有点本末倒置。
普林西普的刺杀引发了一战，但讲道理，让普林西普背一战的锅就说不过去。当然也没人让我背锅，我只是觉着导火索就不应该存在，把潜在的问题扼杀在摇篮中是最好的。因为这次可能是我写string，下次可能就是程序写int，在下次可能就是黑客直接递归删key了，暴露出这么多问题，一个auth能解决？一个disable能解决？
服务发现本来就很复杂，etcd都能被打挂，zk也不是万能钥匙啊，况且还不分区。这么关键的核心服务连这种异常都hold不住，也难怪天天修bug。
我肯定是要被批评的，因为我写的key嘛，就像上次99时候DNS挂了，一个更改防火墙的DBA背锅一样。
几点经验教训：
 对核心数据的权限控制：什么样的人能干什么样的事儿，该干什么样的事儿要有明确的划分。 对数据的隔离和分区：不要把鸡蛋放在一个篮子里，老生常谈了。 核心程序的code review和至少80%的单元测试：不要让这么低级的异常把整个系统搞挂。 关键数据的备份要做好：实在解决不了问题咱回滚数据行不行？ 还是告警，还是日志，错误处理不是说说那么简单的。 服务发现是个大事儿：别用zk，用etcd。 哪怕是内部服务，也不能完全可信，尤其是带有网络调用的，数据验证是基本操作。 出问题是好事儿，跌倒也是好事儿，但出了问题不总结就不是好事儿，总结不到位也不是好事儿，就像明明是脚上扎了钉子，却一直想办法如何修路。早点暴露问题就能早点修复，以免在更严重的时期触发问题；早点跌倒就会早点反思，而不是拖着一只被扎穿的脚彳亍。 最重要的一点也是针对我的一点，没事儿别瞎搞，对于不了解的东西要如履薄冰，如坐针毡，没准你就是最后一块被抽掉的积木。  </description>
    </item>
    
    <item>
      <title>Read-and-Write-in-High-Concurrency</title>
      <link>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</link>
      <pubDate>Fri, 02 Feb 2018 19:17:28 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</guid>
      <description>问题 今天遇到一个高并发读写的问题。
具体的场景是，有一个“策略”的集合，然后每秒有很多消息进来，每一条消息都要匹配有没有对应的策略，如果有的话就应用策略（更改消息的某个属性），没有的话就返回。
抽象来看，就是在N多读的同时，怎样去写数据。
一开始我的方法是策略存在数组里，消息来了就去遍历数组，如果匹配到了就返回对应的规则。这种方法最笨，因为每一条消息过来，我都要去循环遍历整个数组，如果数组很长的话（有很多规则），那么带来的无谓开销会很大，复杂度为O(n)。
而且还一个问题，如果在range数组的时候，数组发生了变化，那么就会读到错误的值，或者崩溃。
一种解决的方法是，在遍历之前，首先拷贝一份新的，遍历新的策略数组，而不是原有的全局变量。这种方法的问题在于，每次匹配规则的时候，都要进行一次拷贝，虽然简单，也能解决问题，但，太浪费资源，而且很丑。
最终的思路是，用哈希表的方式去匹配策略，从复杂度上来看是O(1)的操作，但问题在于并发读写哈希表。
复现 为了更容易的表示问题，用了大量的并发读写（实际情况没有下面那么频繁，写操作要比读操作少得多得多）：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;time&amp;#34; ) var biu map[int]int func read() { for i := 0; i &amp;lt; len(biu); i++ { if biu[i] != i { fmt.Printf(&amp;#34;%d != %d\n&amp;#34;, biu[i], i) panic(&amp;#34;!&amp;#34;) } } } func write() { for i := 0; i &amp;lt; time.Now().Second(); i++ { biu[i] = i } } func main() { biu = make(map[int]int) go func() { for { go write() time.</description>
    </item>
    
    <item>
      <title>Kafka-Timestamp-and-Nginx-Udp-Proxy</title>
      <link>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</link>
      <pubDate>Wed, 24 Jan 2018 21:09:11 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/kafka-timestamp-and-nginx-udp-proxy/</guid>
      <description>昨天遇到一个kafka lib的问题, 明明kafka版本是1.0, 发消息的时候也带上时间戳了, 但接收的时候就是看不到时间戳, timestamp为-1.
如果你也是用的sarama, 记着要在配置的时候制定kafka版本,不然就默认用最小的版本了, 而最小版本(0.8)是没有时间戳功能的, 所以即使后台的kafka是最新的1.0, 那也收不到timestamp, 因为sarama发的时候就没带, 协议版本里没有.
解决方法是, 指定kafka的版本:
config := sarama.NewConfig() config.Version = sarama.V1_0_0_0 // HERE! // sarama.WaitForLocal sarama.WaitForAll sarama.NoResponse config.Producer.RequiredAcks = conf.AckMode 还需要注意的一点是:
// Timestamp is the timestamp assigned to the message by the broker. This // is only guaranteed to be defined if the message was successfully // delivered, RequiredAcks is not NoResponse, and the Kafka broker is at // least version 0.</description>
    </item>
    
    <item>
      <title>Go-TrimLeft-and-TrimPrefix</title>
      <link>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</link>
      <pubDate>Wed, 24 Jan 2018 20:38:42 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</guid>
      <description>今天忽然发现了一个bug，其实是我自己的错误啦，不过也可以甩锅给文档……
简而言之，是用法错误，看这样一个例子：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/key&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }key Program exited. https://play.golang.org/p/Hn8-iVEUi-W
没毛病是吧？
如果你也觉着没毛病，那你也错啦！
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/sugar&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }ugar Program exited. https://play.golang.org/p/_-3V1PgLwjh
喂喂喂！怎么把我的sugar的s给吃了，这是bug吧！
嗯。的确是bug，不过，“凡事总要先从自身找原因”:
 func TrimLeft
func TrimLeft(s string, cutset string) string
TrimLeft returns a slice of the string s with all leading Unicode code points contained in cutset removed.</description>
    </item>
    
  </channel>
</rss>