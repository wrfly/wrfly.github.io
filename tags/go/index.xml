<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>go on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/tags/go/</link>
    <description>Recent content in go on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 08 Dec 2018 16:48:09 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/tags/go/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multi-Error-Handling</title>
      <link>https://wrfly.kfd.me/posts/multi-error-handling/</link>
      <pubDate>Sat, 08 Dec 2018 16:48:09 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/multi-error-handling/</guid>
      <description>Several weeks ago, I was asked to implement a &amp;ldquo;checker&amp;rdquo; that should check multiple errors (or conditions) at one time, once there is an error returned by one of the check, the whole check process should be terminated and returns the error (the first error).
Then after I wrote it in the project, I refactored it and published in github, https://github.com/wrfly/check
This package is quite simple, https://godoc.org/github.com/wrfly/check, it only got two functions:</description>
    </item>
    
    <item>
      <title>golang: netgo vs cgo</title>
      <link>https://wrfly.kfd.me/posts/golang-netgo-vs-cgo/</link>
      <pubDate>Tue, 31 Jul 2018 23:00:06 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-netgo-vs-cgo/</guid>
      <description>由于alpine很小巧, 其经常被拿来当作容器应用运行时的基础镜像, 但是稍不注意, 就会踩一个坑.
一般来讲, 如果我们想改变某个域名的地址, 除了更改DNS记录, 那就只有更改hosts文件了, 由于容器的便捷性, 我们总是会更改容器的hosts文件, 不管是通过 --add-host也好, 挂载hosts文件进去也好, 总之, 是更改这个文件.
问题发现 然鹅! 有时候竟然会出现更改hosts文件无效的情况! 明明在里面ping是可以的呀! 为什么程序就不走这个IP呢? (默认golang程序)
经过仔细的排查发现, 是golang程序的问题&amp;hellip; 中间过程略去不表&amp;hellip;
alpine由于是个小巧的linux系统, 里面没有 /etc/nsswitch.conf 文件, 默认编译的go程序(netgo)会首先去查找这个文件, 判断dns查找顺序, 如果找不到这个文件, 那就 &amp;ldquo;自行决断&amp;rdquo;, 去DNS服务器里查找了. 所以如果想解决这个问题, 可以在image里面把这个文件给 &amp;ldquo;补上&amp;rdquo;: 可以看这个PR
关于nsswitch.conf的详细介绍, 可以看这里 man 或者 man nsswitch.conf
实验验证 下面有个小实验:
package main import &amp;#34;net&amp;#34; const host = &amp;#34;www.google.com&amp;#34; func main() { println(host) ips, err := net.LookupIP(host) if err != nil { panic(err) } for _, ip := range ips { println(&amp;#34;lookup:&amp;#34; + ip.</description>
    </item>
    
    <item>
      <title>Go-Tips-Token-Bucket</title>
      <link>https://wrfly.kfd.me/posts/go-tips-token-bucket/</link>
      <pubDate>Sat, 28 Jul 2018 23:29:49 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-tips-token-bucket/</guid>
      <description>前几天写了一个docker registry的lib https://github.com/wrfly/reglib
然后在写example的时候发现并发太高, 把registry搞得502了, 然后就想办法怎么限制一下这个并发. 其实这个问题在之前面试的时候被问到过, 当时回答的不好. 因为一般的限流令牌桶的话都是有一个时间往里补充的, 我当时还没接触过别的, 所以就比较尴尬.
但是那天下午忽然就想到可以用channel去实现这个令牌桶, 但是操作恰好相反, 操作的时候不是取出令牌再进行下一步, 而是往里放, 对应的, 操作完成也不是再还回令牌, 而是取出.
可以看一个简单的例子:
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) func main() { count := 15 limit := 5 var wg sync.WaitGroup wg.Add(count) tbChan := make(chan struct{}, limit) for index := 0; index &amp;lt; count; index++ { go func(index int) { tbChan &amp;lt;- struct{}{} defer func() { &amp;lt;-tbChan }() defer wg.Done() fmt.Printf(&amp;#34;this is %d\n&amp;#34;, index) time.</description>
    </item>
    
    <item>
      <title>What-is-a-Goroutine, seriously</title>
      <link>https://wrfly.kfd.me/posts/what-is-a-goroutine-seriously/</link>
      <pubDate>Mon, 02 Jul 2018 12:29:07 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/what-is-a-goroutine-seriously/</guid>
      <description>拖延了很久的一篇. 借面试的机会, 梳理一遍.
 init and pstree 在系统中, 所有的执行和操作都是通过进程实现的. 当我们按下电源按钮的那一刻, 就相当于盘古开天辟地了, 从此, 所有的进程都会&amp;rdquo;依附于&amp;rdquo;一个叫init进程的特殊进程. 关于Linux初始化init进程, 可以看下这一个系列:
浅析 Linux 初始化 init 系统:
 Sysvinit UpStart Systemd  我们可以通过 pstree 来查看进程之间的关系.
Process and Thread 严格意义上讲, 并没有进程,线程的区别, Linus 在一篇邮件中表达了自己的观点: http://lkml.iu.edu/hypermail/linux/kernel/9608/0191.html
不管是Process,还是Threads, 都是一些可被执行的实体,context of execution (COE) or independent sequences of execution.
但如果说区别, 进程之间不会共享内存(一般来讲), 而线程之间则会共享进程的内存. 线程是进程资源的子集, 他们唯一的区别在于是否共享了资源.
 That state includes things like CPU state (registers etc), MMU state (page mappings), permission state (uid, gid) and various &amp;ldquo;communication states&amp;rdquo; (open files, signal handlers etc).</description>
    </item>
    
    <item>
      <title>Golang-Append</title>
      <link>https://wrfly.kfd.me/posts/golang-append/</link>
      <pubDate>Sun, 24 Jun 2018 21:26:01 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-append/</guid>
      <description>这几天在准备面试, 想起之前被问到的一个全排序的问题, 今天用golang写了一遍, 无意中发现了一个冷门的知识(或者叫坑).
看下面一段代码:
var NUMS = []int{1, 2, 3} func main() { x1 := NUMS[:1] x2 := NUMS[2:] fmt.Println(x1, x2, NUMS) fmt.Println(append(x1, x2...)) fmt.Println(x1, x2, NUMS) } 大脑编译一下, 结果会是什么?
[1] [3] [1 2 3] [1 3] [1] [3] [1 2 3] 是上面这样吗?
首先截取了 NUMS 的第一个作为 x1(1), 然后截取 NUMS 的第二个之后的, 作为 x2(3), 然后输出 x1, x2 和 NUMS; 然后将 x1 x2 连接起来输出(1 3); 然后重复第一次的输出.
好像没问题吧?
但是.
https://play.golang.org/p/CugWJ9tP34Q
结果却有点出人意料:
[1] [3] [1 2 3] [1 3] [1] [3] [1 3 3] 为什么 NUMS 会被修改呢?</description>
    </item>
    
    <item>
      <title>Golang-Maaaap</title>
      <link>https://wrfly.kfd.me/posts/golang-maaaap/</link>
      <pubDate>Thu, 07 Jun 2018 21:53:55 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-maaaap/</guid>
      <description>下周出去玩.
 从备忘里翻出一个话题. 也是曾经遇到的问题. 在这里记录一下, 希望能给他人帮助.
https://stackoverflow.com/questions/32751537/why-do-i-get-a-cannot-assign-error-when-setting-value-to-a-struct-as-a-value-i
https://stackoverflow.com/questions/17438253/access-struct-in-map-without-copying
Problem 先来看一段小代码:
package main import &amp;#34;fmt&amp;#34; type person struct { name string age int } func main() { m := map[int]person{} for i := 0; i &amp;lt; 5; i++ { m[i] = person{ name: fmt.Sprintf(&amp;#34;person:%d&amp;#34;, i), } } for i := 0; i &amp;lt; 5; i++ { m[i].age = i } for _, p := range m { fmt.Println(p.name, p.age) } } https://play.</description>
    </item>
    
    <item>
      <title>Go-Scheduler</title>
      <link>https://wrfly.kfd.me/posts/go-scheduler/</link>
      <pubDate>Thu, 22 Feb 2018 18:13:29 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-scheduler/</guid>
      <description>翻译自: http://morsmachine.dk/go-scheduler
虽然是13年的一篇文章, 但还是值得一读的.
介绍 Go 1.1版本一个大的改动是由Dmitry Vyukov做的新的调度器. 新的调度器在性能上为go程序并行提供了&amp;rdquo;戏剧性&amp;rdquo;的提升(总之是很牛), 然后自己又没别的事情做, 就想着写一写这个新的调度器(或者说思路, 方法, 概念 balabala).
这篇文章中的大部分内容都在这个原始文档中提出了, 这个文档写的很全面, 但是太&amp;rdquo;技术&amp;rdquo;了. (我感觉也太技术了, 云里雾里的, 所以作者在这里打算通俗的讲一讲)
关于新的调度器, 你需要知道的都在那篇原始文档里, 但是这篇文章有图~ 所以更清晰一点.
带有调度器的Go运行时(go runtime)需要什么 在我们深入了解调度器之前, 我们需要理解为什么它是必须的. 为什么在操作系统可以帮你调度线程的时候, 你仍然需要一个用户空间下的调度器.
POSIX线程API对于现存的Unix进程模型来说更像是一个逻辑扩展, 于此, 对于线程的控制, 更接近与对进程的控制. 线程拥有其自己的信号标志位, 可以被CPU协同分配, 可以被放进cgroups中管理, 可以被查询其所用的资源等. 这些控件对于go程序调度goroutine来说, 增加了很多无谓的功能和开销, 并且当线程数达到100,000时, 开销会迅速增加.
另一个问题是, 基于Go的语言模型, 系统不能创建可被通知的调度决定(informed scheduling decisions). 比如, Go在垃圾回收时, 需要让所有线程停止, 并且线程使用的内存需要连贯. 这就涉及到, 等到所有正在运行的线程达到一个能使内存连贯的点.
当你有很多线程需要被调度到一个随机的点的时候, 唯一的机会是你要等到大多数线程都达到某个内存连贯的点. 对于Go调度器来说, 它知道哪里的内存是连续的, 所以就能作出这样的决定. 这就意味着, 当gc的时候(stw), 我们只需要等待那些正在被CPU运行的线程即可.
角色一览 对于线程来说, 有三种常见的模型. 一个是 N:1, 即多个用户空间下的线程运行在同一个CPU核心上. 这种模型的优点是, 可以在多个线程间快速的切换上下文, 但缺点也很明显, 无法利用多核系统的优点.</description>
    </item>
    
    <item>
      <title>Discover-sync.Pool</title>
      <link>https://wrfly.kfd.me/posts/discover-sync.pool/</link>
      <pubDate>Tue, 20 Feb 2018 20:05:06 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/discover-sync.pool/</guid>
      <description>其实很久之前就用到了这个东西，起因是collecter程序占用太多内存了，然后就用sync.Pool复用额外消耗的一次性内存，避免GC周期太长使内存来不及释放而导致的OOM。
https://golang.org/pkg/sync/#Pool
简单的说，就是在一个池子里放了一些“东西”，这些东西是某种特殊的类型，用的时候需要指定。
池子会随着你的取用而扩张，比如说，池子里面放了扳手（扳手池），现在有10个工人依次取用，当第一个人取的时候，“扳手池”发现没有扳手，ok，new一个出来; 当第一个工人用完了的时候，把扳手放回扳手池，然后第二个人取的时候，扳手池就直接返回那个扳手就可以了。嗯……如果第一个工人没有归还呢，那么扳手池就要重新new一个扳手了，也就是这种情况：10个工人同时取用扳手，那么扳手池就得new10个新的出来了。
一个测试：
https://gist.github.com/wrfly/7de7f1e0c87860aa2f92dc6ed64cb75b
Makefile：
.PHONY: build run test NAME := $(shell basename `pwd`) build: go build run: ./$(NAME) test: build run go tool pprof -lines $(NAME) mem.porf  上面那个gist中，有几个测试，在这种情况下：
wg.Add(alloc) for i := 0; i &amp;lt; alloc; i++ { go func(num int) { // justMake() 	// bufPoolGet() 	bufPoolGetAndPut(num) // bufPoolSleepAndGetAndPut(num) 	wg.Done() }(i) } 也就是拿了接着放回去的时候，结果如下：
➜ syncPool_mem_usage_test make run go build ./syncPool_mem_usage_test newmake: 4 reused: 9996 reused slice(maybe equal to newmake) len: 0 ➜ syncPool_mem_usage_test  也就是说，新分配了4个1e6长度的[]byte，其余的都是复用的。</description>
    </item>
    
    <item>
      <title>RWMutex-and-sync.Map</title>
      <link>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</link>
      <pubDate>Sat, 03 Feb 2018 20:34:48 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</guid>
      <description>In the last post, I noted a problem of read-and-write in high-cocurrency situation and finally chose to use sync.Map. This post I will make a comparation between map with RWMutex and sync.Map.
Read-or-Write Test Code package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) type rwMap struct { data map[int]int m sync.RWMutex } var ( rwm = rwMap{data: make(map[int]int)} end = int(1e7) syncM sync.Map ) func main() { fmt.Println(&amp;#34;write test&amp;#34;) // rw map 	start := time.</description>
    </item>
    
    <item>
      <title>Read-and-Write-in-High-Concurrency</title>
      <link>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</link>
      <pubDate>Fri, 02 Feb 2018 19:17:28 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</guid>
      <description>问题 今天遇到一个高并发读写的问题。
具体的场景是，有一个“策略”的集合，然后每秒有很多消息进来，每一条消息都要匹配有没有对应的策略，如果有的话就应用策略（更改消息的某个属性），没有的话就返回。
抽象来看，就是在N多读的同时，怎样去写数据。
一开始我的方法是策略存在数组里，消息来了就去遍历数组，如果匹配到了就返回对应的规则。这种方法最笨，因为每一条消息过来，我都要去循环遍历整个数组，如果数组很长的话（有很多规则），那么带来的无谓开销会很大，复杂度为O(n)。
而且还一个问题，如果在range数组的时候，数组发生了变化，那么就会读到错误的值，或者崩溃。
一种解决的方法是，在遍历之前，首先拷贝一份新的，遍历新的策略数组，而不是原有的全局变量。这种方法的问题在于，每次匹配规则的时候，都要进行一次拷贝，虽然简单，也能解决问题，但，太浪费资源，而且很丑。
最终的思路是，用哈希表的方式去匹配策略，从复杂度上来看是O(1)的操作，但问题在于并发读写哈希表。
复现 为了更容易的表示问题，用了大量的并发读写（实际情况没有下面那么频繁，写操作要比读操作少得多得多）：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;time&amp;#34; ) var biu map[int]int func read() { for i := 0; i &amp;lt; len(biu); i++ { if biu[i] != i { fmt.Printf(&amp;#34;%d != %d\n&amp;#34;, biu[i], i) panic(&amp;#34;!&amp;#34;) } } } func write() { for i := 0; i &amp;lt; time.Now().Second(); i++ { biu[i] = i } } func main() { biu = make(map[int]int) go func() { for { go write() time.</description>
    </item>
    
    <item>
      <title>Go-TrimLeft-and-TrimPrefix</title>
      <link>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</link>
      <pubDate>Wed, 24 Jan 2018 20:38:42 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</guid>
      <description>今天忽然发现了一个bug，其实是我自己的错误啦，不过也可以甩锅给文档……
简而言之，是用法错误，看这样一个例子：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/key&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }key Program exited. https://play.golang.org/p/Hn8-iVEUi-W
没毛病是吧？
如果你也觉着没毛病，那你也错啦！
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/sugar&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }ugar Program exited. https://play.golang.org/p/_-3V1PgLwjh
喂喂喂！怎么把我的sugar的s给吃了，这是bug吧！
嗯。的确是bug，不过，“凡事总要先从自身找原因”:
 func TrimLeft
func TrimLeft(s string, cutset string) string
TrimLeft returns a slice of the string s with all leading Unicode code points contained in cutset removed.</description>
    </item>
    
    <item>
      <title>Global-Counter-in-Programming</title>
      <link>https://wrfly.kfd.me/posts/global-counter-in-programming/</link>
      <pubDate>Tue, 02 Jan 2018 00:18:50 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/global-counter-in-programming/</guid>
      <description>今天去颐和园玩来着，看到官方开的溜冰场，虽然50一位，但也值了。非常开心。 &amp;mdash; 我
 前些日子给QiMen添加新功能，需要统计实时状态，状态中有个请求数量，所以需要一个计数器。
如果是一般的计数器还好，写一个全局变量一个一个加就可以了，但是由于是高并发的服务，很多线程都回去加加减减，所以全局变量的方法是不可取的。
那既然有高并发，我给这个变量加个锁好了。也不可以，因为是高并发，可以假设每秒有10k请求，那就需要Lock Unlock 10k次，显然是对资源的浪费，很没有必要。 所以加锁的方案也不可行。
既然这样，那不如加个队列吧，一头写，另一头读。这个方案貌似可以，但计数器的实现就变成了“队列”+“计数器”，感觉工程量有点大。所以还是pass了。
emmm，今天吃饭等号的时候，跟阿杰讨论了下，他说他们的方案有一种是依靠外部redis单线程的优势，用redis的counter来加加减减。好像能够解决问题，毕竟redis还是靠得住的，然而，网络IO貌似有点高，而且引入了新的redis组件，如果排除掉网络IO的影响，如果有10个QiMen，每个QiMen每秒10k的消息，那就是每秒100k的操作，不知道redis能不能抗住。（也不确定redis cluster搞不搞的定这个counter）可能是大家的业务场景不一样，所以杰师傅考虑的“统计数据外置”在我的情境下是没必要的，因为我只需要一个总的结果，每隔一段时间反馈给我就好。但大多数情况下，统计数据外置还是政治正确的，这样可以解耦数据，使服务无状态。
然后在golang example中其实就有一个counter，用的是atomic的原子特性。（话说这里有一个插曲，golang的playground中，给每个程序限制了一个线程运行，因为每次跑goroutine都会得到正确的结果，不论加不加锁，也不论加不加atomic，猜测playground在运行时添加了 GOMAXPROCS=1）
var counter uint64 = 0 atomic.AddUint64(&amp;amp;counter, 1) 但这个counter也不能满足我的要求，因为我其实是不知道有多少个counter的，比如请求来源有10个（以IP划分），那我就有10个counter，所以在事先不知道有多少个counter的情况下，这种方案也“貌似”被pass了。（其实在prometheus中，exporter所用的counter内部实现也是atomic）
第一反应是用map:
counter := make(map[string]uint64, 0) if _, ok := counter[source]; !ok { counter[source] = 0 } c := counter[source] atomic.AddUint64(&amp;amp;c, 1) // error here 然而，map里面不让取地址，因为是哈希表，里面的东西会经常变，没有一个准确的地址。（可以搜一下大佬们对此的讨论，如果感兴趣的话）
但是。凡事总有个但是。imcom哥提供了这样一种方案。
counter := make(map[string]*uint64, 0) 最终的counter还是哈希表，但内容是一个指针，这样atomic就可以对其操作。如果想读取值的话，就atomic.LoadUint64一下。还可以在上层封装一个，在一个struct中既创建一个普通的map，又创建一个指针的map，增删全在指针map上进行，读取呢，则先Load到普通的map，然后就可以进行之后的操作了，比如映射json。
type Counter struct{ IP	map[string]uint64 `json:&amp;#34;ip&amp;#34;` IPCounter map[string]*uint64 `json:&amp;#34;-&amp;#34;` } 问题解决。
对于atomic的扩展阅读：sync/atomic - 原子操作</description>
    </item>
    
  </channel>
</rss>