<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Go on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/tags/go/</link>
    <description>Recent content in Go on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 20 Feb 2018 20:05:06 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/tags/go/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Discover-sync.Pool</title>
      <link>https://wrfly.kfd.me/posts/discover-sync.pool/</link>
      <pubDate>Tue, 20 Feb 2018 20:05:06 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/discover-sync.pool/</guid>
      <description>其实很久之前就用到了这个东西，起因是collecter程序占用太多内存了，然后就用sync.Pool复用额外消耗的一次性内存，避免GC周期太长使内存来不及释放而导致的OOM。
https://golang.org/pkg/sync/#Pool
简单的说，就是在一个池子里放了一些“东西”，这些东西是某种特殊的类型，用的时候需要指定。
池子会随着你的取用而扩张，比如说，池子里面放了扳手（扳手池），现在有10个工人依次取用，当第一个人取的时候，“扳手池”发现没有扳手，ok，new一个出来; 当第一个工人用完了的时候，把扳手放回扳手池，然后第二个人取的时候，扳手池就直接返回那个扳手就可以了。嗯……如果第一个工人没有归还呢，那么扳手池就要重新new一个扳手了，也就是这种情况：10个工人同时取用扳手，那么扳手池就得new10个新的出来了。
一个测试：
https://gist.github.com/wrfly/7de7f1e0c87860aa2f92dc6ed64cb75b
Makefile：
.PHONY: build run test NAME := $(shell basename `pwd`) build: go build run: ./$(NAME) test: build run go tool pprof -lines $(NAME) mem.porf  上面那个gist中，有几个测试，在这种情况下：
wg.Add(alloc) for i := 0; i &amp;lt; alloc; i++ { go func(num int) { // justMake() 	// bufPoolGet() 	bufPoolGetAndPut(num) // bufPoolSleepAndGetAndPut(num) 	wg.Done() }(i) } 也就是拿了接着放回去的时候，结果如下：
➜ syncPool_mem_usage_test make run go build ./syncPool_mem_usage_test newmake: 4 reused: 9996 reused slice(maybe equal to newmake) len: 0 ➜ syncPool_mem_usage_test  也就是说，新分配了4个1e6长度的[]byte，其余的都是复用的。</description>
    </item>
    
    <item>
      <title>RWMutex-and-sync.Map</title>
      <link>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</link>
      <pubDate>Sat, 03 Feb 2018 20:34:48 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/rwmutex-and-sync.map/</guid>
      <description>In the last post, I noted a problem of read-and-write in high-cocurrency situation and finally chose to use sync.Map. This post I will make a comparation between map with RWMutex and sync.Map.
Read-or-Write Test Code package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) type rwMap struct { data map[int]int m sync.RWMutex } var ( rwm = rwMap{data: make(map[int]int)} end = int(1e7) syncM sync.Map ) func main() { fmt.Println(&amp;#34;write test&amp;#34;) // rw map 	start := time.</description>
    </item>
    
    <item>
      <title>Read-and-Write-in-High-Concurrency</title>
      <link>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</link>
      <pubDate>Fri, 02 Feb 2018 19:17:28 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/read-and-write-in-high-concurrency/</guid>
      <description>问题 今天遇到一个高并发读写的问题。
具体的场景是，有一个“策略”的集合，然后每秒有很多消息进来，每一条消息都要匹配有没有对应的策略，如果有的话就应用策略（更改消息的某个属性），没有的话就返回。
抽象来看，就是在N多读的同时，怎样去写数据。
一开始我的方法是策略存在数组里，消息来了就去遍历数组，如果匹配到了就返回对应的规则。这种方法最笨，因为每一条消息过来，我都要去循环遍历整个数组，如果数组很长的话（有很多规则），那么带来的无谓开销会很大，复杂度为O(n)。
而且还一个问题，如果在range数组的时候，数组发生了变化，那么就会读到错误的值，或者崩溃。
一种解决的方法是，在遍历之前，首先拷贝一份新的，遍历新的策略数组，而不是原有的全局变量。这种方法的问题在于，每次匹配规则的时候，都要进行一次拷贝，虽然简单，也能解决问题，但，太浪费资源，而且很丑。
最终的思路是，用哈希表的方式去匹配策略，从复杂度上来看是O(1)的操作，但问题在于并发读写哈希表。
复现 为了更容易的表示问题，用了大量的并发读写（实际情况没有下面那么频繁，写操作要比读操作少得多得多）：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;time&amp;#34; ) var biu map[int]int func read() { for i := 0; i &amp;lt; len(biu); i++ { if biu[i] != i { fmt.Printf(&amp;#34;%d != %d\n&amp;#34;, biu[i], i) panic(&amp;#34;!&amp;#34;) } } } func write() { for i := 0; i &amp;lt; time.Now().Second(); i++ { biu[i] = i } } func main() { biu = make(map[int]int) go func() { for { go write() time.</description>
    </item>
    
    <item>
      <title>Go-TrimLeft-and-TrimPrefix</title>
      <link>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</link>
      <pubDate>Wed, 24 Jan 2018 20:38:42 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/go-trimleft-and-trimprefix/</guid>
      <description>今天忽然发现了一个bug，其实是我自己的错误啦，不过也可以甩锅给文档……
简而言之，是用法错误，看这样一个例子：
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/key&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }key Program exited. https://play.golang.org/p/Hn8-iVEUi-W
没毛病是吧？
如果你也觉着没毛病，那你也错啦！
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;strings&amp;#34; ) func main() { str := &amp;#34;/some/sugar&amp;#34; fmt.Println(strings.TrimLeft(str, &amp;#34;/some&amp;#34;)) }ugar Program exited. https://play.golang.org/p/_-3V1PgLwjh
喂喂喂！怎么把我的sugar的s给吃了，这是bug吧！
嗯。的确是bug，不过，“凡事总要先从自身找原因”:
 func TrimLeft
func TrimLeft(s string, cutset string) string
TrimLeft returns a slice of the string s with all leading Unicode code points contained in cutset removed.</description>
    </item>
    
    <item>
      <title>Global-Counter-in-Programming</title>
      <link>https://wrfly.kfd.me/posts/global-counter-in-programming/</link>
      <pubDate>Tue, 02 Jan 2018 00:18:50 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/global-counter-in-programming/</guid>
      <description>今天去颐和园玩来着，看到官方开的溜冰场，虽然50一位，但也值了。非常开心。 &amp;mdash; 我
 前些日子给QiMen添加新功能，需要统计实时状态，状态中有个请求数量，所以需要一个计数器。
如果是一般的计数器还好，写一个全局变量一个一个加就可以了，但是由于是高并发的服务，很多线程都回去加加减减，所以全局变量的方法是不可取的。
那既然有高并发，我给这个变量加个锁好了。也不可以，因为是高并发，可以假设每秒有10k请求，那就需要Lock Unlock 10k次，显然是对资源的浪费，很没有必要。 所以加锁的方案也不可行。
既然这样，那不如加个队列吧，一头写，另一头读。这个方案貌似可以，但计数器的实现就变成了“队列”+“计数器”，感觉工程量有点大。所以还是pass了。
emmm，今天吃饭等号的时候，跟阿杰讨论了下，他说他们的方案有一种是依靠外部redis单线程的优势，用redis的counter来加加减减。好像能够解决问题，毕竟redis还是靠得住的，然而，网络IO貌似有点高，而且引入了新的redis组件，如果排除掉网络IO的影响，如果有10个QiMen，每个QiMen每秒10k的消息，那就是每秒100k的操作，不知道redis能不能抗住。（也不确定redis cluster搞不搞的定这个counter）可能是大家的业务场景不一样，所以杰师傅考虑的“统计数据外置”在我的情境下是没必要的，因为我只需要一个总的结果，每隔一段时间反馈给我就好。但大多数情况下，统计数据外置还是政治正确的，这样可以解耦数据，使服务无状态。
然后在golang example中其实就有一个counter，用的是atomic的原子特性。（话说这里有一个插曲，golang的playground中，给每个程序限制了一个线程运行，因为每次跑goroutine都会得到正确的结果，不论加不加锁，也不论加不加atomic，猜测playground在运行时添加了 GOMAXPROCS=1）
var counter uint64 = 0 atomic.AddUint64(&amp;amp;counter, 1) 但这个counter也不能满足我的要求，因为我其实是不知道有多少个counter的，比如请求来源有10个（以IP划分），那我就有10个counter，所以在事先不知道有多少个counter的情况下，这种方案也“貌似”被pass了。（其实在prometheus中，exporter所用的counter内部实现也是atomic）
第一反应是用map:
counter := make(map[string]uint64, 0) if _, ok := counter[source]; !ok { counter[source] = 0 } c := counter[source] atomic.AddUint64(&amp;amp;c, 1) // error here 然而，map里面不让取地址，因为是哈希表，里面的东西会经常变，没有一个准确的地址。（可以搜一下大佬们对此的讨论，如果感兴趣的话）
但是。凡事总有个但是。imcom哥提供了这样一种方案。
counter := make(map[string]*uint64, 0) 最终的counter还是哈希表，但内容是一个指针，这样atomic就可以对其操作。如果想读取值的话，就atomic.LoadUint64一下。还可以在上层封装一个，在一个struct中既创建一个普通的map，又创建一个指针的map，增删全在指针map上进行，读取呢，则先Load到普通的map，然后就可以进行之后的操作了，比如映射json。
type Counter struct{ IP	map[string]uint64 `json:&amp;#34;ip&amp;#34;` IPCounter map[string]*uint64 `json:&amp;#34;-&amp;#34;` } 问题解决。
对于atomic的扩展阅读：sync/atomic - 原子操作</description>
    </item>
    
  </channel>
</rss>