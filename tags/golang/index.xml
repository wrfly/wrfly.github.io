<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>golang on wrfly&#39;s blog</title>
    <link>https://wrfly.kfd.me/tags/golang/</link>
    <description>Recent content in golang on wrfly&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 30 Sep 2019 00:41:31 +0800</lastBuildDate>
    
	<atom:link href="https://wrfly.kfd.me/tags/golang/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Mendel - A/B Testing Platform (Part 1)</title>
      <link>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p1/</link>
      <pubDate>Mon, 30 Sep 2019 00:41:31 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p1/</guid>
      <description>离上一篇文章已经过了好久了，能够记起还有这么一个坑没填，并且能够记得要些什么，纯属不易…… 这篇主要讲一下系统实现的时候遇到的问题以及我是如何解决的，当中会有疏漏的地方，也欢迎看到这篇文章并且感兴趣的人一起讨论。
流量分布 这个系统最最主要的目的和功能就是分配流量，所以如何分配流量，就成为了首先要面临的问题。
我们的配置分为两种，基础配置和实验配置，那么随之的，流量也分为两种：基础流量和实验流量。对于一个国家的请求而言，如果把流量按照一百份来划分，用了10%的流量做实验，那么就需要在这些请求中挑出10%出来，让其走实验的配置，剩余的90%则按照基础配置进行选择（在某个layer中运用某种算法）。
这其中有一个要求，即流量不能随机划分，不能让用户感知到变化，上一秒是根据base config进行的选择，出现了ABCD，下一秒也要按照base config，不能表现不一致。
这一点我们通过fnv哈希算法来实现，将用户的country，session和uid作为输入，确保三要素相同的情况下，得到的hash分布是随机的。
当然这里也有一些细节，比如既要支持按照session划分，又要按照uid划分，在对一致性要求没那么高的时候，还可以加上hour作为第四因素，变相放大实验流量，得到更多结果。
所以解决方案为：
fnv(country + session + uid [+ hour]) % 100 的出一个数字，通过这个数字判定是base还是experiment
实验选择 接着上面，在得到一个[0,99)的数字以后，怎么就能确定是base还是exp呢？
如果base的config占80%, exp1占10%，exp2占5%，exp3占5%，如何确定一个请求（hash = 55）在哪里呢？
最简单的方法，用一个map[uint8]string来存储没一个数字所对应的实验ID或者BaseID，每次请求过来都用这个map查一下。
但我有点嫌慢，就选择了另外一种方式：bitmap
我们有100份流量，但是数字最大到int64，所以就需要两个int64来存储我们的实验信息，如果一个数字是45,那么就选择第一个bitmap，如果是76,就选择第二个。
具体操作是将bitmap右移[&amp;gt;&amp;gt;]hash位，在与1与或[&amp;amp;&amp;amp;]，如果是真，那么就在这个实验里，如果是假，就换下一个实验的bitmap进行同样操作。
这样能够比map的方式快50%
流量分布不均匀 这其实是自己挖的一个坑，因为所有的流量都会分布到[0,99)上，所以我在存储的时候用了uint8, 认为这个数字已经足够用了，但实际上，虽然它够用，但模100之后并不平均，因为uint8的范围是[0,128), 将这一批平均分布的数字模100之后的分布则必然不平均，[0,28)在[0,99)内占了两份。
// Hash returns a hash value of [0,100) func Hash(x string) uint8 { hash := fnv.New32a() hash.Write([]byte(x)) return uint8(hash.Sum32()) % 100 } 所以简单粗暴的方式是更改一下取模顺序，我们认为hash.Sum32()是平均的，那么其mod 100也是平均的，则最后类型转换成uint8则也是平均的了。
// Hash returns a hash value of [0,100) func Hash(x string) uint8 { hash := fnv.</description>
    </item>
    
    <item>
      <title>Mendel - A/B Testing Platform (Part 0)</title>
      <link>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p0/</link>
      <pubDate>Sun, 07 Jul 2019 13:53:31 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/mendel-ab-testing-platform-p0/</guid>
      <description>背景 &amp;amp; 前言 其实很早之前就想重写我们的AB testing系统，因为现在的程序很简陋（虽然是很久之前某位大哥写的，而且也能凑合用），不支持分国家测试（这种测试很重要），每个返回的结果都会带上很长一串的配置代码（比如，Testing=abpha,Rank=abcde,Threshold=0.5&amp;hellip;.，巨长无比，比我都长），看起来非常罗嗦，也不友好，我们用什么测试都明文暴露出去了，并且现有程序很Java，晦涩难懂，不敢维护。本着破旧立新的原则，搜了一圈没有开源方案能够满足我们的要求，只得自己写一个。
历时两个月：写代码一个月，五千多行，期间还掺杂着乱七八糟的事儿；部署测试一个月，没有QA资源给我们测试，只能自己测，在mesos上部署还搞了一周，DB，DNS，deploy.json, 超级烦。
不过好歹，七月份终于上线了，又经过了两周的线上测试，修了几个bug，现在看起来还不错。
一些简单的功能：
 支持多个项目：不同项目有自己不同的配置和实验 支持多service绑定测试：同一个项目中有多个不同的service 配置/实验区分国家： 不同的国家有不同的配置/测试 可根据UserID，SessionID等维度划分实验流量 支持配置权重：同一个配置有多个值，值附带权重属性 配置多版本，实验多版本：不同版本同时保存，互不冲突 实时分析试验结果：不必麻烦数据团队隔夜处理数据，计算CTR  大概有36个API，因为没有前端（PM画了界面，但是没人有时间写），我还写了一个简单的Python Cli程序。对了，程序语言是Golang，当然。
我给系统起名叫 Mendel，但是PM的PRD上叫MVT，Multiple Variable Testing system，这个名字个人感觉很low，不如“孟德尔”有意思。不过我才是写代码的人，我想怎么写就怎么写（傲娇）。
架构 整个系统分为四个部分：
 Mendel Server Mendel Assistant Database  MySQL DB etcd influxDB   Mendel Lib  Mendel Server 负责处理所有API请求，基本上就是CURD，接受请求，验证处理，存入DB，同步到etcd上。这部分写起来其实很讨厌，虽然简单，但是太麻烦，每一个接口都要有CURD，我原来想用MongoDB作为后端存储，但是公司不提供MongoDB的维护，自己用容器风险又太大（没人维护），只能用MySQL，然后用text保存数据。因为如果想要满足设计，配置就需要相当复杂，区分国家，配置权重，service关系，不同配置的层级关系，用MySQL字段保存的话太麻烦了，也不能满足多种层级。其实写好一个CURD也不是容易事，太多细节需要考虑，不仅仅是麻烦的问题。
Mendel Assistant
这个组件是为了实时分析实验结果的，通过实时读取Tracking数据，获取Tracking里面的AB Testing Siganture，解开Signature，写入InfluxDB和暴露metrics给Prometheus，计算不同实验的CTR，对比效果。Mendel Server部署一个就够了，他只是个CURD Server，但这个Assistant需要部署很多个，毕竟Tracking的数据量太大了。
Database
MySQL，所有的“配置”都在这里，包括各个项目的元信息，基础配置，实验配置，流量分配（以后还会有用户管理，没来得及实现用户系统）
Etcd，也是存储各种的配置信息，不过它还有解藕client与db的功能，也是作为一个缓存的存在，也作为发布/更新配置与实验的中间件（归功与etcd v3的gRPC长连接watch）（唉，但是自从接手etcd以来，已经踩了好多坑了……）。
InfluxDB，存储我们的试验结果，因为其支持多个field，不像projetheus那样只能有固定个数的lables。需要多个fields的原因是，系统运行中，配置会随时更改，增加或者删除某一些层级配置，比如有时候想测试某种算法的好坏，但一旦测试完成，就有可能把这一层的配置删掉，固定用测试结果优秀的算法，或者动态更改配置的阈值，有时候是0.2,有时候是0.3,有时候是0.5,这都是不可预测的，所以只能用InfluxDB解决。且InfluxDB聚合能力还可以，虽然是单点（公司不给出钱买商业版），但我还没遇到什么瓶颈，如果数据丢了，重新set一下Kafka的Offset，再跑一边写进去就是了。
Mendel Lib
原来的设计是没一个请求进来，我们用一个TrackingID去标记，然后在TrackingID上append不同的配置，写入一个KV存储中，但是Lead说，公司的Codis不靠谱，没有一个靠谱的持久化KV存储，如果TrackingID的数据丢了就麻烦了，所以最后折中了一个Signature的方案，从很长的配置细节变成一个能够自解的signature，并且signature对外不可读。对于signature的介绍将在本文最后进行。
其实用trackingID的方式是最好的，这样不仅可以用来标记数据，也可以用这个ID对请求进行追踪，比如在什么时候产生了访问，返回了多少广告，关键词是什么，在什么时候点击的，点击了第几个位置，这个位置的广告的平均CTR是多少，rank分是多少，similarity是多少，这些数据对于我们分析用户行为，提升算法精准度，都是有很大帮助的，但是无奈，只能用signature去自解AB Testing的数据。无奈。
配置结构 在这套系统中，有几个关键的概念：
 project: 拥有独立流量（独立请求和返回结果）的业务线称为一个项目 base_config: 项目的基础配置，一个项目配置中，可以包含多个服务  service: 服务，一个服务配置中，包含多个模块配置 module:：模块，具体表现为程序的某个模块，可以包含多个层级配置 layer: 在程序中的特定层，具体为某一个模块中的某一个步骤 config value: 对层级的配置，包括这个层中，对国家的配置值，不同值的权重等   traffic slots: 每一个项目拥有自己独立的流量分配，每一个国家也有独立的流量分配（保证了实验的可以在国家的维度进行） experiment: 每一个实验本质上也是一套配置，不过是建立在基础配置之上的特定修改，而且实验配置不能超出基础配置，一定是修改。实验可以根据不同的维度划分，比如根据UID划分，根据SessionID划分，但是它们一定是共享整个项目的流量的，相同国家内的流量不能够重叠交叉。实验中对于layer的配置也可以有多种，且对于每一种配置值，其权重都是相同的（有别于base config中的config value）。对于实验没有修改的layer，当base config中layer有变更时，experiment也要同步进行修改，更新，下发。  对于base config和expeirment config，所有数据都有唯一的ID和版本号，保证了不会有数据丢失，方便回滚和审计。</description>
    </item>
    
    <item>
      <title>Golang-Open-File</title>
      <link>https://wrfly.kfd.me/posts/golang-open-file/</link>
      <pubDate>Tue, 16 Apr 2019 22:13:25 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-open-file/</guid>
      <description>很久之前遇到的问题了，清任务的时候这个write排第一，就写一下。
起因 要做一个临时的告警系统，又不想引入DB，也不想用bolt，不想引入额外的包，所以就傻了吧唧的自己用文件实现一个简单的DB。
（现在回想起来真的很没必要，老老实实用boltdb多好，简单快速搞得定）
每一条告警都有自己的hash，如果在文件中没有发现这条hash，则发送告警并将hash写入文件，如果发现了则跳过，目的是防止重复告警。
很简单的逻辑，但bug是，还是会发送重复的告警，也就是说，如果这条告警半小时之前发过了，那么半小时之后还会再发。（程序是用cron job跑的，每半小时跑一次，查询各个状态，如果指标低于阈值则告警）
经过 程序的某个部分是这样写的：
fileName := &amp;#34;/tmp/alert-history.txt&amp;#34; f, err := os.OpenFile(fileName, os.O_RDWR, 0644) if err != os.ErrNotExist { panic(fmt.Sprintf(&amp;#34;open file %s error: %s&amp;#34;, fileName, err)) } else { f, err = os.Create(fileName) if err != nil { panic(fmt.Sprintf(&amp;#34;create file %s error: %s&amp;#34;, fileName, err)) } } defer f.Close() // detect alarm 	// ...  // if alarm not in file 	// then send alarm and 	// write its hash to the file 	// .</description>
    </item>
    
    <item>
      <title>Golang-For-Loop</title>
      <link>https://wrfly.kfd.me/posts/golang-for-loop/</link>
      <pubDate>Sun, 25 Nov 2018 14:34:14 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/golang-for-loop/</guid>
      <description>😀
Recently my colleague meets a problem with golang for loop. He ranged an array of User, wanted to filter some user and append them (use pointer to get their address) to another user array, but finally got an array contains the same user.
Some thing like this:
newUsers := []*User{} for _, u := range UserArray{ if checkFailed(u){ continue } newUsers = append(newUsers, &amp;amp;u) } The newUsers got the same user with different indexes.</description>
    </item>
    
    <item>
      <title>Gracefully-Shutdown</title>
      <link>https://wrfly.kfd.me/posts/gracefully-shutdown/</link>
      <pubDate>Wed, 03 Jan 2018 23:03:07 +0800</pubDate>
      
      <guid>https://wrfly.kfd.me/posts/gracefully-shutdown/</guid>
      <description>时间是宇宙唯一的资源 &amp;mdash; 我
 因为上周准备code review的时候遇到了这个问题，大家也给出了一些建议，和imcom哥也进行了一番讨论，了解了golang中context的一些用法，在这里记录备份一下，也希望对别人有所帮助。
首先简单的阐述一下问题是什么。golang编程，经常会go出去一些goroutine，抛出去不难，关键要看怎么收回来，因为程序不仅有启动，还要有退出，不管是正常退出还是非正常退出，总得有一个clean up的过程，不然就会导致程序不可控，引发非正常退出，数据丢失或者脏数据等一些乱七八糟的问题。
所以我们要在程序退出的时候对申请的资源进行释放，主动关闭已经建立的连接，完成正在进行的工作，然后退出程序。
github上有很多公开的让http服务graceful退出的lib，好像某些框架也提供了gracefulstop的方法。其本质就是停止服务，关闭端口，拒绝了新来的请求，然后把手头正在进行的请求处理完，或者设置一个超时时间强制结束正在进行的请求，然后server就stop了。这里我不谈论这个，因为有很多框架都自带了这个功能，而且不限于http服务。
但有个东西是要参考的，golang中的context。写过http服务的人都知道，每个请求的request中都带了一个context，一开始我是不知道这是做什么用的，但每个东西都有其用法，这个context的用法就是让server端和client联系起来的一个上下文，也可以理解为纽带。最基础的，比如client发出了一个请求，但是由于某种原因（网络断了，链接丢了，客户端主动关闭了，Ctrl+C了），这个连接断了，那么server端还要继续处理么，肯定不要了嘛，不然发给谁，给鬼啊，所以server就要根据这个context进行下一步处理，如果context已经done了，那么这个请求就可以直接return了。
说起来会很枯燥，看代码（跟上面说的server无关了哈，有点偏）：
package main import ( &amp;#34;context&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; ) var wg sync.WaitGroup func main() { ctx, cancel := context.WithCancel(context.Background()) defer cancel() n := 3 wg.Add(n) for i := 0; i &amp;lt; n; i++ { go testContext(ctx) } time.Sleep(2100 * time.Millisecond) cancel() wg.Wait() } func testContext(ctx context.Context) { defer wg.Done() defer fmt.Println(&amp;#34;stop&amp;#34;) tk := time.NewTicker(time.Second) defer tk.Stop() for { select { case &amp;lt;-ctx.</description>
    </item>
    
  </channel>
</rss>